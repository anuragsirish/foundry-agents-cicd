# Dual-Agent Evaluation Architecture

## Overview

This architecture implements **automated dual-agent evaluation** for AI agent quality assurance. On every pull request, the system evaluates both a baseline agent and a V2 agent, compares their performance across multiple quality metrics, and presents the results for manual review.

### Key Principles

1. **Always Pass** - Workflow never fails automatically; developers decide based on metrics
2. **Dual Evaluation** - Both baseline and V2 agents evaluated independently
3. **Clear Indicators** - ğŸŸ¢ improvements, ğŸ”´ regressions, ğŸŸ¡ neutral changes
4. **Full Transparency** - All results available in PR comments, Actions summary, and artifacts
5. **Secure by Default** - Uses Azure federated credentials (OIDC), no secrets in code

### Agents

| Agent | Description | Variable |
|-------|-------------|----------|
| **Baseline** | Current production or reference agent | `AGENT_ID_BASELINE` |
| **V2** | New or experimental agent being tested | `AGENT_ID_V2` |

## High-Level Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DUAL-AGENT EVALUATION WORKFLOW                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Developer                 GitHub                    Azure AI
        â”‚                        â”‚                          â”‚
        â”‚  1. Create PR          â”‚                          â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                          â”‚
        â”‚                        â”‚                          â”‚
        â”‚                        â”‚  2. Trigger Workflow     â”‚
        â”‚                        â”‚     (agent-eval-on-pr)   â”‚
        â”‚                        â”‚                          â”‚
        â”‚                        â”‚  3. Checkout code        â”‚
        â”‚                        â”‚                          â”‚
        â”‚                        â”‚  4. Authenticate Azure   â”‚
        â”‚                        â”‚     (OIDC/Federated)     â”‚
        â”‚                        â”‚                          â”‚
        â”‚                        â”‚  5. Evaluate Baseline    â”‚
        â”‚                        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
        â”‚                        â”‚     (AGENT_ID_BASELINE)  â”‚
        â”‚                        â”‚                          â”‚  6. Execute baseline
        â”‚                        â”‚                          â”‚     with test queries
        â”‚                        â”‚  7. Baseline results     â”‚
        â”‚                        â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚                        â”‚                          â”‚
        â”‚                        â”‚  8. Evaluate V2 Agent    â”‚
        â”‚                        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
        â”‚                        â”‚     (AGENT_ID_V2)        â”‚
        â”‚                        â”‚                          â”‚  9. Execute V2
        â”‚                        â”‚                          â”‚     with test queries
        â”‚                        â”‚  10. V2 results          â”‚
        â”‚                        â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚                        â”‚                          â”‚
        â”‚                        â”‚  11. Compare metrics     â”‚
        â”‚                        â”‚      (baseline vs V2)    â”‚
        â”‚                        â”‚                          â”‚
        â”‚                        â”‚  12. Generate outputs:   â”‚
        â”‚                        â”‚      â€¢ PR comment        â”‚
        â”‚                        â”‚      â€¢ Actions summary   â”‚
        â”‚                        â”‚      â€¢ Artifacts         â”‚
        â”‚                        â”‚                          â”‚
        â”‚  13. View results      â”‚                          â”‚
        â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                          â”‚
        â”‚     (Always âœ… PASS)   â”‚                          â”‚
        â”‚                        â”‚                          â”‚
        â”‚  14. Review & Merge    â”‚                          â”‚
        â”‚     (Manual decision)  â”‚                          â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                          â”‚
        â”‚                        â”‚                          â”‚
        â–¼                        â–¼                          â–¼


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      BASELINE INITIALIZATION                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ First Run        â”‚
â”‚ (No baseline)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€> PR Workflow runs
         â”‚   â””â”€> Detects: baseline_exists=false
         â”‚       â””â”€> Shows "First evaluation run" message
         â”‚
         â”œâ”€> PR merged to main
         â”‚   â””â”€> update-baseline.yml triggers
         â”‚       â””â”€> Runs evaluation
         â”‚           â””â”€> Creates baseline_metrics.json
         â”‚               â””â”€> Commits to repository
         â”‚
         â””â”€> Future PRs
             â””â”€> Compare against baseline


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     COMPARISON & DECISION FLOW                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

            Evaluate Baseline Agent
                      â”‚
                      â–¼
            Evaluate V2 Agent
                      â”‚
                      â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Compare Metrics      â”‚
            â”‚ (Baseline vs V2)     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â–¼                 â–¼                 â–¼
                 ğŸŸ¢ Improved        ğŸŸ¡ Neutral       ğŸ”´ Regressed
                 (V2 > Baseline)    (Similar)        (V2 < Baseline)
                       â”‚                 â”‚                 â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚ Quality Metrics: â”‚
                      â”‚ â€¢ Relevance      â”‚
                      â”‚ â€¢ Coherence      â”‚
                      â”‚ â€¢ Fluency        â”‚
                      â”‚ â€¢ Groundedness   â”‚
                      â”‚ â€¢ Tool Call Acc. â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚ Generate Outputs â”‚
                      â”‚ â€¢ PR comment     â”‚
                      â”‚ â€¢ Actions summaryâ”‚
                      â”‚ â€¢ Artifacts      â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                      âœ… Workflow PASSES
                      (Always succeeds)
                               â”‚
                               â–¼
                      Manual merge decision
                      (Developer reviews metrics)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FILE STRUCTURE & FLOW                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Repository Root
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ agent-eval-on-pr.yml â”€â”€â”€â”€â”€â”€â”  ğŸ¤– ACTIVE (Dual-agent evaluation)
â”‚   â”‚   â”‚                           â”‚
â”‚   â”‚   â”œâ”€> Triggers on PR          â”‚
â”‚   â”‚   â”œâ”€> Evaluates baseline â”€â”€â”€â”€â”€â”¼â”€> evaluation_results/baseline/
â”‚   â”‚   â”œâ”€> Evaluates V2 agent â”€â”€â”€â”€â”€â”¼â”€> evaluation_results/v2/
â”‚   â”‚   â”œâ”€> Compares metrics        â”‚
â”‚   â”‚   â”œâ”€> Posts PR comment        â”‚
â”‚   â”‚   â”œâ”€> GitHub Actions summary  â”‚
â”‚   â”‚   â”œâ”€> Uploads artifacts       â”‚
â”‚   â”‚   â””â”€> Always passes âœ…        â”‚
â”‚   â”‚                                â”‚
â”‚   â””â”€â”€ agent-eval-on-pr-official.yml  âŒ DISABLED (Microsoft action)
â”‚       â”‚
â”‚       â””â”€> Manual dispatch only
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ local_agent_eval.py
â”‚   â”‚   â””â”€> Used by workflows and local testing
â”‚   â”‚
â”‚   â””â”€â”€ initialize_baseline.py
â”‚       â””â”€> One-time setup
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ agent-eval-data.json
â”‚       â””â”€> Test queries (10 samples)
â”‚
â”œâ”€â”€ evaluation_results/
â”‚   â”œâ”€â”€ baseline/
â”‚   â”‚   â””â”€â”€ baseline_results.json       ğŸ“¦ Workflow artifacts
â”‚   â”‚
â”‚   â”œâ”€â”€ v2/
â”‚   â”‚   â””â”€â”€ v2_results.json             ğŸ“¦ Workflow artifacts
â”‚   â”‚
â”‚   â””â”€â”€ agent_eval_output/              âŒ Not committed
â”‚       â””â”€â”€ eval-output.json            (local runs only)
â”‚
â””â”€â”€ .env
    â””â”€> Local development only


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA FLOW DETAILS                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input                 Processing              Output
â”€â”€â”€â”€â”€                 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€

Test Queries          Agent Execution         Conversation Logs
(data/agent-          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>          (messages array)
eval-data.json)              â”‚
                             â”‚
                             â–¼
                      
                      Evaluator Models        Quality Scores
                      (GPT-4o judges)   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>  (0-5 scale)
                             â”‚
                             â”‚
                             â–¼
                      
                      Aggregate Metrics       Average Scores
                      (mean calculation) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>  per metric
                             â”‚
                             â”‚
                             â–¼
                      
Current PR            Baseline Comparison     Comparison Results
Results          <â”€â”€> Baseline Metrics  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>  (diff & %)
                             â”‚
                             â”‚
                             â–¼
                      
                      Quality Gate Logic      Pass/Fail Decision
                      (check thresholds) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>  + PR comment


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      METRIC COMPARISON LOGIC                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

For each metric:

    baseline_value = 4.20  (from baseline agent)
    v2_value = 4.50        (from V2 agent)
    
    diff = v2_value - baseline_value
         = 4.50 - 4.20
         = +0.30
    
    # Determine status indicator
    if abs(diff) < 0.1:     # Less than 0.1 difference
        status = ğŸŸ¡           # Neutral (no significant change)
    elif diff > 0:           # Positive difference
        status = ğŸŸ¢           # Improvement
    else:                    # Negative difference
        status = ğŸ”´           # Regression
    
    # Calculate percentage change (for display)
    if baseline_value > 0:
        pct_change = (diff / baseline_value) Ã— 100
                   = (0.30 / 4.20) Ã— 100
                   = +7.1%
    
    # Note: Workflow always passes regardless of status
        âœ… PASS
        emoji = ğŸŸ¢
    else:                  # Within Â±5%
        âœ… PASS
        emoji = ğŸŸ¡


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TIMELINE: TYPICAL PR LIFECYCLE                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

T+0:00    Developer creates PR
          â”‚
T+0:05    GitHub triggers workflow
          â”‚
T+0:30    Workflow starts
          â”œâ”€ Checkout code (10s)
          â”œâ”€ Setup Python (20s)
          â”œâ”€ Install deps (30s)
          â”œâ”€ Azure login (10s)
          â””â”€ Check baseline (5s)
          â”‚
T+1:45    Run evaluation
          â”œâ”€ Initialize client (10s)
          â”œâ”€ Execute 10 queries (60s)
          â”‚  â””â”€ ~6s per query
          â””â”€ Run evaluators (120s)
             â””â”€ 8 evaluators Ã— 10 queries
          â”‚
T+3:45    Compare & generate comment (15s)
          â”‚
T+4:00    Post PR comment
          â”‚
T+4:05    Quality gate check (5s)
          â”‚
          â””â”€> âœ… PASS or âŒ FAIL
          â”‚
[Manual]  Code review by team
          â”‚
[Manual]  PR approved & merged
          â”‚
T+0:00    Push to main triggers baseline update
          â”‚
T+4:00    New baseline saved and committed
          â”‚
          â””â”€> Ready for next PR


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         SYSTEM COMPONENTS                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GitHub Actions â”‚
â”‚                  â”‚
â”‚  Workflows:      â”‚
â”‚  â€¢ PR eval       â”‚
â”‚  â€¢ Baseline      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Uses
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Python Script  â”‚
â”‚                  â”‚
â”‚  local_agent_    â”‚
â”‚  eval.py         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Connects to
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Azure AI        â”‚       â”‚  Azure OpenAI    â”‚
â”‚  Project Client  â”‚       â”‚  (Evaluators)    â”‚
â”‚                  â”‚       â”‚                  â”‚
â”‚  â€¢ Agent         â”‚â—„â”€â”€â”€â”€â”€â–ºâ”‚  â€¢ GPT-4o        â”‚
â”‚  â€¢ Threads       â”‚       â”‚  â€¢ Judges        â”‚
â”‚  â€¢ Messages      â”‚       â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Produces
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Evaluation     â”‚
â”‚   Results        â”‚
â”‚                  â”‚
â”‚  â€¢ Metrics JSON  â”‚
â”‚  â€¢ Baseline      â”‚
â”‚  â€¢ Comparisons   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Points

1. **Automatic**: Triggers on every PR to main
2. **Smart**: Detects baseline existence
3. **Comprehensive**: 8+ quality metrics evaluated
4. **Transparent**: Results visible in PR comment
5. **Gated**: Blocks PRs if quality degrades >5%
6. **Self-updating**: Baseline updates on merge
7. **Historical**: Full git history of baselines
8. **Flexible**: Thresholds and metrics customizable

## Metrics Evaluated

| Category | Metrics |
|----------|---------|
| **Quality** (Block PRs) | Relevance, Coherence, Fluency, Groundedness, Tool Call Accuracy, Intent Resolution, Task Adherence, Similarity |
| **Performance** (Track only) | Response Time, Completion Tokens, Prompt Tokens |

## Next Steps

1. Review [CICD_PIPELINE.md](./CICD_PIPELINE.md) for setup
2. Configure GitHub variables
3. Initialize baseline
4. Create test PR
5. Monitor results
