name: 🤖 Comprehensive Agent Evaluation (Quality + Safety + Red Team)

on:
  pull_request:
    types: [opened, synchronize, reopened]
    paths:
      - 'agent-setup/**'
      - 'data/**'
      - 'scripts/**'
      - '.github/workflows/agent-evaluation-unified.yml'

permissions:
  id-token: write
  contents: read
  pull-requests: write

jobs:
  evaluate-baseline:
    name: 🎯 Baseline Agent
    runs-on: ubuntu-latest
    
    steps:
      - name: 🤖 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🐍 Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: 🔐 Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: 🎯 Evaluate Baseline Agent (Quality)
        env:
          AZURE_CLIENT_ID: ${{ vars.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ vars.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_DEPLOYMENT_NAME: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          AGENT_ID_BASELINE: ${{ vars.AGENT_ID_BASELINE }}
        run: |
          echo "🎯 Running quality evaluation on Baseline Agent..."
          python scripts/local_quality_eval.py
      
      - name: 📤 Upload Baseline Quality Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: baseline-results
          path: evaluation_results/quality_eval_output/
          retention-days: 30

  evaluate-v2:
    name: 🚀 V2 Agent
    runs-on: ubuntu-latest
    
    steps:
      - name: 🤖 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🐍 Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: 🔐 Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: 🚀 Evaluate V2 Agent (Quality)
        env:
          AZURE_CLIENT_ID: ${{ vars.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ vars.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_DEPLOYMENT_NAME: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          AGENT_ID_BASELINE: ${{ vars.AGENT_ID_V2 }}
        run: |
          echo "🚀 Running quality evaluation on V2 Agent..."
          python scripts/local_quality_eval.py
      
      - name: 📤 Upload V2 Quality Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: v2-results
          path: evaluation_results/quality_eval_output/
          retention-days: 30

  compare-and-report:
    name: 📊 Comprehensive Report (Quality + Safety + Red Team)
    needs: [evaluate-baseline, evaluate-v2, safety-baseline, safety-v2, redteam-baseline, redteam-v2]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: 🤖 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 📥 Download Baseline Results
        uses: actions/download-artifact@v4
        with:
          name: baseline-results
          path: baseline-results
      
      - name: 📥 Download V2 Results
        uses: actions/download-artifact@v4
        with:
          name: v2-results
          path: v2-results
      
      - name: � Download Baseline Safety Results
        uses: actions/download-artifact@v4
        with:
          name: baseline-safety-results
          path: baseline-safety-results
        continue-on-error: true
      
      - name: 📥 Download V2 Safety Results
        uses: actions/download-artifact@v4
        with:
          name: v2-safety-results
          path: v2-safety-results
        continue-on-error: true
      
      - name: 📥 Download Baseline Red Team Results
        uses: actions/download-artifact@v4
        with:
          name: baseline-redteam-results
          path: baseline-redteam-results
        continue-on-error: true
      
      - name: 📥 Download V2 Red Team Results
        uses: actions/download-artifact@v4
        with:
          name: v2-redteam-results
          path: v2-redteam-results
        continue-on-error: true
      
      - name: �📊 Compare and Generate Report
        id: compare
        run: |
          python - <<'EOF'
          import json
          import os
          from pathlib import Path
          
          def load_metrics(file_path):
              """Load metrics from evaluation output file."""
              try:
                  print(f"Attempting to load: {file_path}")
                  if not os.path.exists(file_path):
                      print(f"File not found: {file_path}")
                      # List directory contents for debugging
                      dir_path = os.path.dirname(file_path)
                      if os.path.exists(dir_path):
                          print(f"Contents of {dir_path}:")
                          for item in os.listdir(dir_path):
                              print(f"  - {item}")
                      return {}
                  
                  with open(file_path, 'r') as f:
                      data = json.load(f)
                      metrics = data.get('metrics', {})
                      print(f"Loaded {len(metrics)} metrics from {file_path}")
                      return metrics
              except Exception as e:
                  print(f"Error loading {file_path}: {e}")
                  import traceback
                  traceback.print_exc()
                  return {}
          
          def get_primary_metric(metrics_dict, base_key):
              """Get the primary metric value (without prefixes like gpt_)."""
              # Try keys in order of preference
              for key in [base_key, f'gpt_{base_key}', f'{base_key}.{base_key}']:
                  if key in metrics_dict:
                      return metrics_dict[key]
              return None
          
          # Load both evaluation results
          baseline_metrics = load_metrics('baseline-results/quality-eval-output.json')
          v2_metrics = load_metrics('v2-results/quality-eval-output.json')
          
          if not baseline_metrics or not v2_metrics:
              print("⚠️ Could not load metrics from both agents")
              print(f"Baseline metrics loaded: {len(baseline_metrics)} keys")
              print(f"V2 metrics loaded: {len(v2_metrics)} keys")
              
              # Create a minimal comparison file so the workflow doesn't fail
              with open('comparison.md', 'w') as f:
                  f.write("## 📊 Comprehensive Evaluation Results\n\n")
                  f.write("⚠️ **Comparison not available** - Could not load metrics from evaluation outputs.\n\n")
                  f.write("Please check the individual job logs for details.\n")
              exit(0)
          
          # Define metric categories
          quality_metrics = [
              'relevance', 'coherence', 'fluency', 'groundedness', 
              'similarity', 'intent_resolution', 'task_adherence'
          ]
          
          tool_metrics = ['tool_call_accuracy']
          
          operational_metrics = [
              'client-run-duration-in-seconds',
              'completion-tokens',
              'prompt-tokens',
              'server-run-duration-in-seconds'
          ]
          
          # Generate comprehensive comparison
          print("\n" + "="*100)
          print("📊 COMPREHENSIVE AGENT EVALUATION COMPARISON")
          print("="*100 + "\n")
          
          comparison_md = "## 📊 Comprehensive Evaluation Results\n\n"
          
          # Quality Metrics Section
          comparison_md += "### 🎯 Quality Metrics (1-5 scale)\n\n"
          comparison_md += "| Metric | Baseline | V2 | Change | Status |\n"
          comparison_md += "|--------|----------|----|---------|---------|\n"
          
          improvements = 0
          regressions = 0
          neutral = 0
          
          print("### 🎯 Quality Metrics")
          print()
          
          for metric in quality_metrics:
              baseline_val = get_primary_metric(baseline_metrics, metric)
              v2_val = get_primary_metric(v2_metrics, metric)
              
              if baseline_val is not None and v2_val is not None:
                  diff = v2_val - baseline_val
                  
                  if abs(diff) < 0.1:
                      status = "🟡"
                      neutral += 1
                  elif diff > 0:
                      status = "🟢"
                      improvements += 1
                  else:
                      status = "🔴"
                      regressions += 1
                  
                  metric_name = metric.replace('_', ' ').title()
                  comparison_md += f"| {metric_name} | {baseline_val:.2f} | {v2_val:.2f} | {diff:+.2f} | {status} |\n"
                  print(f"{metric_name:<25} Baseline: {baseline_val:.2f}  V2: {v2_val:.2f}  Change: {diff:+.2f}  {status}")
          
          print()
          
          # Tool Call Accuracy Section
          comparison_md += "\n### 🛠️ Tool Call Accuracy\n\n"
          comparison_md += "| Metric | Baseline | V2 | Change | Status |\n"
          comparison_md += "|--------|----------|----|---------|---------|\n"
          
          print("### 🛠️ Tool Call Accuracy")
          print()
          
          for metric in tool_metrics:
              baseline_val = get_primary_metric(baseline_metrics, metric)
              v2_val = get_primary_metric(v2_metrics, metric)
              
              if baseline_val is not None and v2_val is not None:
                  diff = v2_val - baseline_val
                  
                  if abs(diff) < 0.05:
                      status = "🟡"
                      neutral += 1
                  elif diff > 0:
                      status = "🟢"
                      improvements += 1
                  else:
                      status = "🔴"
                      regressions += 1
                  
                  metric_name = metric.replace('_', ' ').title()
                  comparison_md += f"| {metric_name} | {baseline_val:.2f} | {v2_val:.2f} | {diff:+.2f} | {status} |\n"
                  print(f"{metric_name:<25} Baseline: {baseline_val:.2f}  V2: {v2_val:.2f}  Change: {diff:+.2f}  {status}")
          
          print()
          
          # Operational Metrics Section
          comparison_md += "\n### ⚡ Operational Metrics\n\n"
          comparison_md += "| Metric | Baseline | V2 | Change | Status |\n"
          comparison_md += "|--------|----------|----|---------|---------|\n"
          
          print("### ⚡ Operational Metrics")
          print()
          
          for metric in operational_metrics:
              full_key = f'operational_metrics.{metric}'
              baseline_val = baseline_metrics.get(full_key)
              v2_val = v2_metrics.get(full_key)
              
              if baseline_val is not None and v2_val is not None:
                  diff = v2_val - baseline_val
                  
                  # For duration and tokens, lower is better
                  if 'duration' in metric or 'tokens' in metric:
                      if abs(diff) < (baseline_val * 0.1):  # Less than 10% change
                          status = "🟡"
                      elif diff < 0:  # Decrease is good
                          status = "🟢"
                      else:  # Increase is bad
                          status = "🔴"
                  else:
                      status = "🟡"
                  
                  metric_name = metric.replace('-', ' ').title()
                  
                  # Format based on metric type
                  if 'duration' in metric:
                      comparison_md += f"| {metric_name} | {baseline_val:.2f}s | {v2_val:.2f}s | {diff:+.2f}s | {status} |\n"
                      print(f"{metric_name:<35} Baseline: {baseline_val:.2f}s  V2: {v2_val:.2f}s  Change: {diff:+.2f}s  {status}")
                  elif 'tokens' in metric:
                      comparison_md += f"| {metric_name} | {baseline_val:.0f} | {v2_val:.0f} | {diff:+.0f} | {status} |\n"
                      print(f"{metric_name:<35} Baseline: {baseline_val:.0f}  V2: {v2_val:.0f}  Change: {diff:+.0f}  {status}")
                  else:
                      comparison_md += f"| {metric_name} | {baseline_val:.2f} | {v2_val:.2f} | {diff:+.2f} | {status} |\n"
                      print(f"{metric_name:<35} Baseline: {baseline_val:.2f}  V2: {v2_val:.2f}  Change: {diff:+.2f}  {status}")
          
          print()
          
          # Binary Aggregates Section (Threshold-based metrics)
          comparison_md += "\n### ✅ Threshold Performance (Pass Rate)\n\n"
          comparison_md += "| Metric | Baseline | V2 | Status |\n"
          comparison_md += "|--------|----------|----|---------|\n"
          
          print("### ✅ Threshold Performance")
          print()
          
          binary_metrics = [k for k in baseline_metrics.keys() if k.endswith('.binary_aggregate')]
          
          for metric_key in sorted(binary_metrics):
              baseline_val = baseline_metrics.get(metric_key)
              v2_val = v2_metrics.get(metric_key)
              
              if baseline_val is not None and v2_val is not None:
                  metric_name = metric_key.replace('.binary_aggregate', '').replace('_', ' ').title()
                  
                  baseline_pct = baseline_val * 100
                  v2_pct = v2_val * 100
                  diff = v2_val - baseline_val
                  
                  if abs(diff) < 0.01:
                      status = "🟡"
                  elif diff > 0:
                      status = "🟢"
                  else:
                      status = "🔴"
                  
                  comparison_md += f"| {metric_name} | {baseline_pct:.0f}% | {v2_pct:.0f}% | {status} |\n"
                  print(f"{metric_name:<30} Baseline: {baseline_pct:.0f}%  V2: {v2_pct:.0f}%  {status}")
          
          print()
          
          # Safety Metrics Section
          def load_safety_metrics(file_path):
              """Load safety metrics from evaluation output file."""
              try:
                  if not os.path.exists(file_path):
                      return None
                  with open(file_path, 'r') as f:
                      data = json.load(f)
                      return data.get('metrics', {})
              except Exception as e:
                  print(f"Could not load safety metrics from {file_path}: {e}")
                  return None
          
          baseline_safety = load_safety_metrics('baseline-safety-results/safety-eval-output.json')
          v2_safety = load_safety_metrics('v2-safety-results/safety-eval-output.json')
          
          if baseline_safety and v2_safety:
              comparison_md += "\n### 🛡️ Safety Metrics (Defect Rate %)\n\n"
              comparison_md += "| Category | Baseline | V2 | Change | Status |\n"
              comparison_md += "|----------|----------|----|---------|---------|\n"
              
              print("### 🛡️ Safety Metrics")
              print()
              
              safety_categories = ['violence', 'sexual', 'self_harm', 'hate_unfairness']
              
              for category in safety_categories:
                  baseline_key = f'content_safety.{category}_defect_rate'
                  v2_key = f'content_safety.{category}_defect_rate'
                  
                  baseline_val = baseline_safety.get(baseline_key)
                  v2_val = v2_safety.get(v2_key)
                  
                  if baseline_val is not None and v2_val is not None:
                      baseline_pct = baseline_val * 100
                      v2_pct = v2_val * 100
                      diff = v2_val - baseline_val
                      diff_pct = diff * 100
                      
                      # Lower defect rate is better
                      if abs(diff) < 0.01:  # Less than 1% change
                          status = "🟡"
                      elif diff < 0:  # Decrease is good
                          status = "🟢"
                      else:  # Increase is bad
                          status = "🔴"
                          regressions += 1
                      
                      category_name = category.replace('_', ' ').title()
                      comparison_md += f"| {category_name} | {baseline_pct:.1f}% | {v2_pct:.1f}% | {diff_pct:+.1f}% | {status} |\n"
                      print(f"{category_name:<25} Baseline: {baseline_pct:.1f}%  V2: {v2_pct:.1f}%  Change: {diff_pct:+.1f}%  {status}")
              
              # Binary aggregate
              baseline_agg = baseline_safety.get('content_safety.binary_aggregate')
              v2_agg = v2_safety.get('content_safety.binary_aggregate')
              
              if baseline_agg is not None and v2_agg is not None:
                  comparison_md += f"| **Overall Safety** | {baseline_agg*100:.0f}% | {v2_agg*100:.0f}% | {(v2_agg-baseline_agg)*100:+.0f}% | {'🟢' if v2_agg >= baseline_agg else '🔴'} |\n"
                  print(f"{'Overall Safety':<25} Baseline: {baseline_agg*100:.0f}%  V2: {v2_agg*100:.0f}%")
              
              print()
          
          # Red Team Metrics Section
          def load_redteam_results(file_path):
              """Load red team evaluation results."""
              try:
                  if not os.path.exists(file_path):
                      return None
                  with open(file_path, 'r') as f:
                      return json.load(f)
              except Exception as e:
                  print(f"Could not load red team results from {file_path}: {e}")
                  return None
          
          baseline_redteam = load_redteam_results('baseline-redteam-results/redteam-summary.json')
          v2_redteam = load_redteam_results('v2-redteam-results/redteam-summary.json')
          
          if baseline_redteam and v2_redteam:
              comparison_md += "\n### 🔴 Red Team Adversarial Testing\n\n"
              comparison_md += "| Metric | Baseline | V2 | Status |\n"
              comparison_md += "|--------|----------|----|---------|\n"
              
              print("### 🔴 Red Team Adversarial Testing")
              print()
              
              baseline_asr = baseline_redteam.get('attack_success_rate', 0) * 100
              v2_asr = v2_redteam.get('attack_success_rate', 0) * 100
              
              baseline_attacks = baseline_redteam.get('total_attacks', 0)
              v2_attacks = v2_redteam.get('total_attacks', 0)
              
              # Lower ASR is better (more resilient)
              if v2_asr < baseline_asr - 1:
                  asr_status = "🟢 More Resilient"
              elif v2_asr > baseline_asr + 1:
                  asr_status = "🔴 Less Resilient"
                  regressions += 1
              else:
                  asr_status = "🟡 Similar"
              
              comparison_md += f"| Attack Success Rate | {baseline_asr:.1f}% | {v2_asr:.1f}% | {asr_status} |\n"
              comparison_md += f"| Total Attacks | {baseline_attacks} | {v2_attacks} | - |\n"
              
              print(f"{'Attack Success Rate':<25} Baseline: {baseline_asr:.1f}%  V2: {v2_asr:.1f}%  {asr_status}")
              print(f"{'Total Attacks':<25} Baseline: {baseline_attacks}  V2: {v2_attacks}")
              
              # Vulnerable categories
              baseline_vuln = baseline_redteam.get('vulnerable_categories', [])
              v2_vuln = v2_redteam.get('vulnerable_categories', [])
              
              comparison_md += f"| Vulnerable Categories | {len(baseline_vuln)} | {len(v2_vuln)} | {'🟢' if len(v2_vuln) <= len(baseline_vuln) else '🔴'} |\n"
              print(f"{'Vulnerable Categories':<25} Baseline: {len(baseline_vuln)}  V2: {len(v2_vuln)}")
              
              if v2_vuln:
                  comparison_md += f"\n**V2 Vulnerabilities**: {', '.join([c.replace('_', ' ').title() for c in v2_vuln])}\n"
              
              print()
          
          # Summary Section
          comparison_md += "\n### 📈 Summary\n\n"
          comparison_md += f"- 🟢 **Improvements**: {improvements}\n"
          comparison_md += f"- 🔴 **Regressions**: {regressions}\n"
          comparison_md += f"- 🟡 **Neutral**: {neutral}\n\n"
          
          comparison_md += "\n**Evaluation Coverage:**\n"
          comparison_md += "- ✅ Quality Metrics (8 evaluators)\n"
          comparison_md += "- ✅ Safety Evaluation (4 risk categories)\n"
          comparison_md += "- ✅ Red Team Testing (65+ attack strategies)\n\n"
          
          if regressions == 0:
              comparison_md += "✅ **No regressions detected** - V2 agent maintains or improves across all dimensions!\n"
          elif improvements > regressions:
              comparison_md += f"⚠️ **Mixed results** - {improvements} improvements vs {regressions} regressions. Review details before merging.\n"
          else:
              comparison_md += f"⚠️ **Concerns detected** - {regressions} regressions across quality/safety/red team. Consider investigating before merging.\n"
          
          print("="*100)
          print(f"📈 Summary: {improvements} improvements, {regressions} regressions, {neutral} neutral")
          print("="*100)
          print()
          
          # Save comparison for PR comment
          with open('comparison.md', 'w') as f:
              f.write(comparison_md)
          
          # Save all metrics for detailed view
          all_metrics_md = "\n### 📋 All Baseline Metrics\n\n"
          all_metrics_md += "<details><summary>Click to expand</summary>\n\n"
          all_metrics_md += "| Metric | Value |\n"
          all_metrics_md += "|--------|-------|\n"
          
          for key in sorted(baseline_metrics.keys()):
              val = baseline_metrics[key]
              if isinstance(val, (int, float)):
                  all_metrics_md += f"| `{key}` | {val:.4f} |\n"
          
          all_metrics_md += "\n</details>\n\n"
          
          all_metrics_md += "### 📋 All V2 Metrics\n\n"
          all_metrics_md += "<details><summary>Click to expand</summary>\n\n"
          all_metrics_md += "| Metric | Value |\n"
          all_metrics_md += "|--------|-------|\n"
          
          for key in sorted(v2_metrics.keys()):
              val = v2_metrics[key]
              if isinstance(val, (int, float)):
                  all_metrics_md += f"| `{key}` | {val:.4f} |\n"
          
          all_metrics_md += "\n</details>\n"
          
          with open('all-metrics.md', 'w') as f:
              f.write(all_metrics_md)
          
          print("✅ Comparison generated successfully!")
          EOF
      
      - name: 💬 Comment on PR
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let comparisonMd = '';
            let allMetricsMd = '';
            
            try {
              comparisonMd = fs.readFileSync('comparison.md', 'utf8');
            } catch (error) {
              comparisonMd = '## 📊 Evaluation Results\n\n⚠️ Comparison could not be generated.';
            }
            
            try {
              allMetricsMd = fs.readFileSync('all-metrics.md', 'utf8');
            } catch (error) {
              allMetricsMd = '';
            }
            
            const artifactsSection = `
            ### 📦 Artifacts
            **Quality Results:**
            - Baseline Quality Results
            - V2 Quality Results
            
            **Safety Results:**
            - Baseline Safety Results
            - V2 Safety Results
            
            **Red Team Results:**
            - Baseline Red Team Results
            - V2 Red Team Results
            
            ---
            *This evaluation includes quality metrics (8 evaluators), safety testing (4 risk categories), and red team adversarial testing (65+ attack strategies).*
            
            *Legend: 🟢 Improved | 🔴 Regressed | 🟡 Neutral*
            `;
            
            const comment = comparisonMd + '\n\n' + allMetricsMd + '\n\n' + artifactsSection;
            
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const existingComment = comments.find(c => 
              c.body.includes('📊 Comprehensive Evaluation Results')
            );
            
            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }
      
      - name: 📋 Post to GitHub Actions Summary
        if: always()
        run: |
          echo "## 🤖 Comprehensive Agent Evaluation (Quality + Safety + Red Team)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f comparison.md ]; then
            cat comparison.md >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            if [ -f all-metrics.md ]; then
              cat all-metrics.md >> $GITHUB_STEP_SUMMARY
            fi
            
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### 📦 Artifacts" >> $GITHUB_STEP_SUMMARY
            echo "- Baseline Results: \`baseline-results\`" >> $GITHUB_STEP_SUMMARY
            echo "- V2 Results: \`v2-results\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ **Comparison not available**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "The comparison step did not generate results. Check the logs above for details." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Troubleshooting" >> $GITHUB_STEP_SUMMARY
            echo "1. Check if both evaluation jobs completed successfully" >> $GITHUB_STEP_SUMMARY
            echo "2. Verify artifacts were uploaded correctly" >> $GITHUB_STEP_SUMMARY
            echo "3. Review the 'Compare and Generate Report' step logs" >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: ✅ Evaluation Complete
        if: always()
        run: |
          echo "✅ Agent quality evaluation completed!"
          echo ""
          echo "ℹ️  Note: This workflow provides comprehensive metrics but does not block PRs."
          echo "   Review all metrics and make an informed decision about merging."

  safety-baseline:
    name: 🛡️ Safety - Baseline
    runs-on: ubuntu-latest
    
    steps:
      - name: 🤖 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🐍 Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: 🔐 Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: 🛡️ Run Safety Evaluation
        env:
          AZURE_CLIENT_ID: ${{ vars.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ vars.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_DEPLOYMENT_NAME: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          AGENT_ID_BASELINE: ${{ vars.AGENT_ID_BASELINE }}
        run: |
          echo "🛡️ Running safety evaluation on Baseline Agent..."
          python scripts/local_safety_eval.py
      
      - name: 📤 Upload Safety Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: baseline-safety-results
          path: evaluation_results/safety_eval_output/
          retention-days: 30

  safety-v2:
    name: 🛡️ Safety - V2
    runs-on: ubuntu-latest
    
    steps:
      - name: 🤖 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🐍 Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: 🔐 Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: 🛡️ Run Safety Evaluation
        env:
          AZURE_CLIENT_ID: ${{ vars.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ vars.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_DEPLOYMENT_NAME: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          AGENT_ID_BASELINE: ${{ vars.AGENT_ID_V2 }}
        run: |
          echo "🛡️ Running safety evaluation on V2 Agent..."
          python scripts/local_safety_eval.py
      
      - name: 📤 Upload Safety Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: v2-safety-results
          path: evaluation_results/safety_eval_output/
          retention-days: 30

  safety-comparison:
    name: 🛡️ Safety Comparison
    needs: [safety-baseline, safety-v2]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: 🤖 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 📥 Download Baseline Safety Results
        uses: actions/download-artifact@v4
        with:
          name: baseline-safety-results
          path: baseline-safety-results
        continue-on-error: true
      
      - name: 📥 Download V2 Safety Results
        uses: actions/download-artifact@v4
        with:
          name: v2-safety-results
          path: v2-safety-results
        continue-on-error: true
      
      - name: 📊 Compare Safety Results
        id: safety-compare
        run: |
          python - <<'EOF'
          import json
          import os
          from pathlib import Path
          
          def load_safety_results(file_path):
              """Load safety evaluation results."""
              try:
                  print(f"Attempting to load: {file_path}")
                  if not os.path.exists(file_path):
                      print(f"File not found: {file_path}")
                      return None
                  
                  with open(file_path, 'r') as f:
                      data = json.load(f)
                      print(f"Loaded safety results from {file_path}")
                      return data
              except Exception as e:
                  print(f"Error loading {file_path}: {e}")
                  import traceback
                  traceback.print_exc()
                  return None
          
          # Try to load both safety results
          baseline_safety = load_safety_results('baseline-safety-results/safety-eval-output.json')
          v2_safety = load_safety_results('v2-safety-results/safety-eval-output.json')
          
          print("\n" + "="*80)
          print("🛡️ SAFETY EVALUATION COMPARISON")
          print("="*80 + "\n")
          
          comparison_md = "## 🛡️ Safety Evaluation Results\n\n"
          
          if baseline_safety is None and v2_safety is None:
              comparison_md += "⚠️ **Safety evaluations did not complete**\n\n"
              comparison_md += "Check the individual safety evaluation job logs for details.\n"
          else:
              # Safety categories to compare
              safety_categories = [
                  'violence', 'sexual', 'self_harm', 'hate_unfairness'
              ]
              
              comparison_md += "| Safety Category | Baseline | V2 | Status |\n"
              comparison_md += "|----------------|----------|----|---------|\n"
              
              for category in safety_categories:
                  baseline_val = None
                  v2_val = None
                  
                  if baseline_safety and 'metrics' in baseline_safety:
                      # Try different key patterns
                      for key in baseline_safety['metrics'].keys():
                          if category in key.lower() and 'defect_rate' in key.lower():
                              baseline_val = baseline_safety['metrics'][key]
                              break
                  
                  if v2_safety and 'metrics' in v2_safety:
                      for key in v2_safety['metrics'].keys():
                          if category in key.lower() and 'defect_rate' in key.lower():
                              v2_val = v2_safety['metrics'][key]
                              break
                  
                  if baseline_val is not None and v2_val is not None:
                      baseline_pct = baseline_val * 100
                      v2_pct = v2_val * 100
                      
                      # Lower defect rate is better
                      if v2_pct < baseline_pct - 1:
                          status = "🟢 Improved"
                      elif v2_pct > baseline_pct + 1:
                          status = "🔴 Regressed"
                      else:
                          status = "🟡 Similar"
                      
                      category_name = category.replace('_', ' ').title()
                      comparison_md += f"| {category_name} | {baseline_pct:.1f}% | {v2_pct:.1f}% | {status} |\n"
                      print(f"{category_name:<25} Baseline: {baseline_pct:.1f}%  V2: {v2_pct:.1f}%  {status}")
                  elif baseline_val is not None:
                      category_name = category.replace('_', ' ').title()
                      baseline_pct = baseline_val * 100
                      comparison_md += f"| {category_name} | {baseline_pct:.1f}% | N/A | ⚠️ Missing |\n"
                  elif v2_val is not None:
                      category_name = category.replace('_', ' ').title()
                      v2_pct = v2_val * 100
                      comparison_md += f"| {category_name} | N/A | {v2_pct:.1f}% | ⚠️ Missing |\n"
              
              comparison_md += "\n*Lower defect rates are better. Rates shown are percentage of responses flagged as potentially harmful.*\n"
          
          print("\n" + "="*80 + "\n")
          
          # Save comparison
          with open('safety-comparison.md', 'w') as f:
              f.write(comparison_md)
          
          print("✅ Safety comparison generated")
          EOF
      
      - name: 💬 Comment Safety Results on PR
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let safetyMd = '';
            try {
              safetyMd = fs.readFileSync('safety-comparison.md', 'utf8');
            } catch (error) {
              safetyMd = '## 🛡️ Safety Evaluation\n\n⚠️ Safety evaluation did not complete.';
            }
            
            const comment = `${safetyMd}
            
            ### 📦 Safety Artifacts
            - **Baseline Safety Results**: Available in workflow artifacts
            - **V2 Safety Results**: Available in workflow artifacts
            
            ---
            *Safety evaluations check for: Violence, Sexual content, Self-harm, and Hate/Unfairness*
            `;
            
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const existingComment = comments.find(c => 
              c.body.includes('🛡️ Safety Evaluation Results')
            );
            
            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }
      
      - name: 📋 Post Safety to GitHub Actions Summary
        if: always()
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f safety-comparison.md ]; then
            cat safety-comparison.md >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### 📦 Safety Artifacts" >> $GITHUB_STEP_SUMMARY
            echo "- Baseline Safety Results: \`baseline-safety-results\`" >> $GITHUB_STEP_SUMMARY
            echo "- V2 Safety Results: \`v2-safety-results\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ **Safety comparison not available**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Check the safety evaluation job logs for details." >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: ✅ Safety Evaluation Complete
        if: always()
        run: |
          echo "✅ Safety evaluation workflow completed!"
          echo ""
          echo "ℹ️  Note: Safety evaluations provide insights but do not block PRs."
          echo "   Review safety metrics and make an informed decision about merging."

  # ============================================================================
  # RED TEAM EVALUATION
  # ============================================================================
  
  redteam-baseline:
    name: 🔴 Red Team - Baseline
    runs-on: ubuntu-latest
    
    steps:
      - name: 🤖 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🐍 Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: 🔐 Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: 🔴 Run Red Team Evaluation
        env:
          AZURE_CLIENT_ID: ${{ vars.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ vars.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_DEPLOYMENT_NAME: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          AGENT_ID_BASELINE: ${{ vars.AGENT_ID_BASELINE }}
          RED_TEAM_MAX_SCENARIOS: 3
        run: |
          echo "🔴 Running red team evaluation on Baseline Agent..."
          echo "Note: Limited to 3 scenarios for CI/CD speed"
          python scripts/local_redteam_eval.py
      
      - name: 📤 Upload Red Team Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: baseline-redteam-results
          path: evaluation_results/redteam_eval_output/
          retention-days: 30

  redteam-v2:
    name: 🔴 Red Team - V2
    runs-on: ubuntu-latest
    
    steps:
      - name: 🤖 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🐍 Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: 🔐 Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: 🔴 Run Red Team Evaluation
        env:
          AZURE_CLIENT_ID: ${{ vars.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ vars.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_DEPLOYMENT_NAME: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          AGENT_ID_BASELINE: ${{ vars.AGENT_ID_V2 }}
          RED_TEAM_MAX_SCENARIOS: 3
        run: |
          echo "🔴 Running red team evaluation on V2 Agent..."
          echo "Note: Limited to 3 scenarios for CI/CD speed"
          python scripts/local_redteam_eval.py
      
      - name: 📤 Upload Red Team Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: v2-redteam-results
          path: evaluation_results/redteam_eval_output/
          retention-days: 30

  redteam-comparison:
    name: 🔴 Red Team Comparison
    needs: [redteam-baseline, redteam-v2]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: 🤖 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 📥 Download Baseline Red Team Results
        uses: actions/download-artifact@v4
        with:
          name: baseline-redteam-results
          path: baseline-redteam-results
        continue-on-error: true
      
      - name: 📥 Download V2 Red Team Results
        uses: actions/download-artifact@v4
        with:
          name: v2-redteam-results
          path: v2-redteam-results
        continue-on-error: true
      
      - name: 📊 Compare Red Team Results
        id: redteam-compare
        run: |
          python - <<'EOF'
          import json
          import os
          from pathlib import Path
          
          def load_redteam_results(file_path):
              """Load red team evaluation results."""
              try:
                  print(f"Attempting to load: {file_path}")
                  if not os.path.exists(file_path):
                      print(f"File not found: {file_path}")
                      return None
                  
                  with open(file_path, 'r') as f:
                      data = json.load(f)
                      print(f"Loaded red team results from {file_path}")
                      return data
              except Exception as e:
                  print(f"Error loading {file_path}: {e}")
                  import traceback
                  traceback.print_exc()
                  return None
          
          # Load both red team results
          baseline_redteam = load_redteam_results('baseline-redteam-results/redteam-summary.json')
          v2_redteam = load_redteam_results('v2-redteam-results/redteam-summary.json')
          
          print("\n" + "="*80)
          print("🔴 RED TEAM EVALUATION COMPARISON")
          print("="*80 + "\n")
          
          comparison_md = "## 🔴 Red Team Adversarial Testing Results\n\n"
          
          if baseline_redteam is None and v2_redteam is None:
              comparison_md += "⚠️ **Red team evaluations did not complete**\n\n"
              comparison_md += "Check the individual red team evaluation job logs for details.\n"
          else:
              comparison_md += "*Red team testing uses 65+ attack strategies including ROT13, Leetspeak, Base64, Unicode attacks, and more.*\n\n"
              
              # Overall metrics
              comparison_md += "### 📊 Overall Attack Success Rate\n\n"
              comparison_md += "| Metric | Baseline | V2 | Status |\n"
              comparison_md += "|--------|----------|----|---------|\n"
              
              if baseline_redteam and v2_redteam:
                  baseline_asr = baseline_redteam.get('attack_success_rate', 0) * 100
                  v2_asr = v2_redteam.get('attack_success_rate', 0) * 100
                  
                  baseline_attacks = baseline_redteam.get('total_attacks', 0)
                  v2_attacks = v2_redteam.get('total_attacks', 0)
                  
                  # Lower ASR is better (more resilient)
                  if v2_asr < baseline_asr - 1:
                      status = "🟢 More Resilient"
                  elif v2_asr > baseline_asr + 1:
                      status = "🔴 Less Resilient"
                  else:
                      status = "🟡 Similar"
                  
                  comparison_md += f"| Attack Success Rate | {baseline_asr:.1f}% | {v2_asr:.1f}% | {status} |\n"
                  comparison_md += f"| Total Attacks Tested | {baseline_attacks} | {v2_attacks} | - |\n"
                  
                  print(f"Attack Success Rate - Baseline: {baseline_asr:.1f}%  V2: {v2_asr:.1f}%  {status}")
                  print(f"Total Attacks - Baseline: {baseline_attacks}  V2: {v2_attacks}")
                  
                  # Safety vulnerabilities by category
                  comparison_md += "\n### 🛡️ Safety Defect Rates by Category\n\n"
                  comparison_md += "| Category | Baseline | V2 | Status |\n"
                  comparison_md += "|----------|----------|----|---------|\n"
                  
                  baseline_safety = baseline_redteam.get('safety_metrics', {})
                  v2_safety = v2_redteam.get('safety_metrics', {})
                  
                  if baseline_safety and v2_safety:
                      for category in ['violence', 'sexual', 'self_harm', 'hate_unfairness']:
                          baseline_data = baseline_safety.get(category, {})
                          v2_data = v2_safety.get(category, {})
                          
                          baseline_rate = baseline_data.get('defect_rate', 0) * 100
                          v2_rate = v2_data.get('defect_rate', 0) * 100
                          
                          baseline_status_emoji = baseline_data.get('status', '')
                          v2_status_emoji = v2_data.get('status', '')
                          
                          # Lower defect rate is better
                          if v2_rate < baseline_rate - 1:
                              status = "🟢 Improved"
                          elif v2_rate > baseline_rate + 1:
                              status = "🔴 Regressed"
                          else:
                              status = "🟡 Similar"
                          
                          category_name = category.replace('_', ' ').title()
                          comparison_md += f"| {category_name} | {baseline_rate:.1f}% {baseline_status_emoji} | {v2_rate:.1f}% {v2_status_emoji} | {status} |\n"
                          print(f"{category_name:<20} Baseline: {baseline_rate:.1f}%  V2: {v2_rate:.1f}%  {status}")
                  
                  # Vulnerable categories
                  baseline_vuln = baseline_redteam.get('vulnerable_categories', [])
                  v2_vuln = v2_redteam.get('vulnerable_categories', [])
                  
                  comparison_md += "\n### ⚠️ Vulnerable Categories\n\n"
                  
                  if not baseline_vuln and not v2_vuln:
                      comparison_md += "✅ **Both agents are resilient** - No vulnerable categories detected in either baseline or V2.\n"
                  else:
                      comparison_md += f"**Baseline**: {', '.join([c.replace('_', ' ').title() for c in baseline_vuln]) if baseline_vuln else 'None'}\n\n"
                      comparison_md += f"**V2**: {', '.join([c.replace('_', ' ').title() for c in v2_vuln]) if v2_vuln else 'None'}\n"
              
              comparison_md += "\n*Lower defect rates and attack success rates indicate better resilience against adversarial attacks.*\n"
          
          print("\n" + "="*80 + "\n")
          
          # Save comparison
          with open('redteam-comparison.md', 'w') as f:
              f.write(comparison_md)
          
          print("✅ Red team comparison generated")
          EOF
      
      - name: 💬 Comment Red Team Results on PR
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let redteamMd = '';
            try {
              redteamMd = fs.readFileSync('redteam-comparison.md', 'utf8');
            } catch (error) {
              redteamMd = '## 🔴 Red Team Evaluation\n\n⚠️ Red team evaluation did not complete.';
            }
            
            const comment = `${redteamMd}
            
            ### 📦 Red Team Artifacts
            - **Baseline Red Team Results**: Available in workflow artifacts
            - **V2 Red Team Results**: Available in workflow artifacts
            
            ### 🔗 Portal Links
            Check the individual job logs for Azure AI Foundry portal links to view detailed attack patterns and responses.
            
            ---
            *Red team evaluations test agent resilience against 65+ adversarial attack strategies*
            `;
            
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const existingComment = comments.find(c => 
              c.body.includes('🔴 Red Team Adversarial Testing Results')
            );
            
            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }
      
      - name: 📋 Post Red Team to GitHub Actions Summary
        if: always()
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f redteam-comparison.md ]; then
            cat redteam-comparison.md >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### 📦 Red Team Artifacts" >> $GITHUB_STEP_SUMMARY
            echo "- Baseline Red Team Results: \`baseline-redteam-results\`" >> $GITHUB_STEP_SUMMARY
            echo "- V2 Red Team Results: \`v2-redteam-results\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ **Red team comparison not available**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Check the red team evaluation job logs for details." >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: ✅ Red Team Evaluation Complete
        if: always()
        run: |
          echo "✅ Red team evaluation workflow completed!"
          echo ""
          echo "📊 Red team testing includes:"
          echo "   • 65+ attack strategies (ROT13, Leetspeak, Base64, etc.)"
          echo "   • Multiple complexity levels (Easy, Moderate, Difficult)"
          echo "   • Safety evaluation of adversarial responses"
          echo ""
          echo "ℹ️  Note: Red team evaluations provide insights but do not block PRs."
          echo "   Review all metrics and make an informed decision about merging."
