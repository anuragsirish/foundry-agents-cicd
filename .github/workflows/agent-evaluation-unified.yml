name: 🤖 Comprehensive Agent Evaluation (Quality + Safety + Red Team)

on:
  pull_request:
    types: [opened, synchronize, reopened]
    paths:
      - 'agent-setup/**'
      - 'data/**'
      - 'scripts/**'
      - '.github/workflows/agent-evaluation-unified.yml'

permissions:
  id-token: write
  contents: read
  pull-requests: write

jobs:
  evaluate-baseline:
    name: 🎯 Baseline Agent
    runs-on: ubuntu-latest
    
    steps:
      - name: 🤖 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🐍 Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: 🔐 Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: 🎯 Evaluate Baseline Agent (Quality)
        env:
          AZURE_CLIENT_ID: ${{ vars.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ vars.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_DEPLOYMENT_NAME: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          AGENT_ID_BASELINE: ${{ vars.AGENT_ID_BASELINE }}
        run: |
          echo "🎯 Running quality evaluation on Baseline Agent..."
          python scripts/local_quality_eval.py
      
      - name: 📤 Upload Baseline Quality Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: baseline-results
          path: evaluation_results/quality_eval_output/
          retention-days: 30

  evaluate-v2:
    name: 🚀 V2 Agent
    runs-on: ubuntu-latest
    
    steps:
      - name: 🤖 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🐍 Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: 🔐 Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: 🚀 Evaluate V2 Agent (Quality)
        env:
          AZURE_CLIENT_ID: ${{ vars.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ vars.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_DEPLOYMENT_NAME: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          AGENT_ID_BASELINE: ${{ vars.AGENT_ID_V2 }}
        run: |
          echo "🚀 Running quality evaluation on V2 Agent..."
          python scripts/local_quality_eval.py
      
      - name: 📤 Upload V2 Quality Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: v2-results
          path: evaluation_results/quality_eval_output/
          retention-days: 30

  compare-and-report:
    name: 📊 Comprehensive Report (Quality + Safety + Red Team)
    needs: [evaluate-baseline, evaluate-v2, safety-baseline, safety-v2, redteam-baseline, redteam-v2]
    runs-on: ubuntu-latest
    if: always() && (needs.evaluate-baseline.result == 'success' || needs.evaluate-baseline.result == 'failure') && (needs.evaluate-v2.result == 'success' || needs.evaluate-v2.result == 'failure')
    
    steps:
      - name: 🤖 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 📥 Download Baseline Results
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: baseline-results
          path: baseline-results
      
      - name: 📥 Download V2 Results
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: v2-results
          path: v2-results
      
      - name: � Download Baseline Safety Results
        uses: actions/download-artifact@v4
        with:
          name: baseline-safety-results
          path: baseline-safety-results
        continue-on-error: true
      
      - name: 📥 Download V2 Safety Results
        uses: actions/download-artifact@v4
        with:
          name: v2-safety-results
          path: v2-safety-results
        continue-on-error: true
      
      - name: 📥 Download Baseline Red Team Results
        uses: actions/download-artifact@v4
        with:
          name: baseline-redteam-results
          path: baseline-redteam-results
        continue-on-error: true
      
      - name: 📥 Download V2 Red Team Results
        uses: actions/download-artifact@v4
        with:
          name: v2-redteam-results
          path: v2-redteam-results
        continue-on-error: true
      
      - name: 📋 Create Missing Result Files
        run: |
          # Create directories if they don't exist
          mkdir -p baseline-results v2-results baseline-safety-results v2-safety-results baseline-redteam-results v2-redteam-results
          
          # Create empty result files if missing
          if [ ! -f baseline-results/quality-eval-output.json ]; then
            echo '{"metrics": {}, "error": "Baseline evaluation failed or did not complete"}' > baseline-results/quality-eval-output.json
            echo "⚠️ Created placeholder for missing baseline-results"
          fi
          
          if [ ! -f v2-results/quality-eval-output.json ]; then
            echo '{"metrics": {}, "error": "V2 evaluation failed or did not complete"}' > v2-results/quality-eval-output.json
            echo "⚠️ Created placeholder for missing v2-results"
          fi
      
      - name: �📊 Compare and Generate Report
        id: compare
        run: |
          python - <<'EOF'
          import json
          import os
          from pathlib import Path
          
          def load_metrics(file_path):
              """Load metrics from evaluation output file."""
              try:
                  print(f"Attempting to load: {file_path}")
                  if not os.path.exists(file_path):
                      print(f"File not found: {file_path}")
                      # List directory contents for debugging
                      dir_path = os.path.dirname(file_path)
                      if os.path.exists(dir_path):
                          print(f"Contents of {dir_path}:")
                          for item in os.listdir(dir_path):
                              print(f"  - {item}")
                      return {}
                  
                  with open(file_path, 'r') as f:
                      data = json.load(f)
                      metrics = data.get('metrics', {})
                      print(f"Loaded {len(metrics)} metrics from {file_path}")
                      return metrics
              except Exception as e:
                  print(f"Error loading {file_path}: {e}")
                  import traceback
                  traceback.print_exc()
                  return {}
          
          def get_primary_metric(metrics_dict, base_key):
              """Get the primary metric value (without prefixes like gpt_)."""
              # Try keys in order of preference
              for key in [base_key, f'gpt_{base_key}', f'{base_key}.{base_key}']:
                  if key in metrics_dict:
                      return metrics_dict[key]
              return None
          
          # Load both evaluation results
          baseline_metrics = load_metrics('baseline-results/quality-eval-output.json')
          v2_metrics = load_metrics('v2-results/quality-eval-output.json')
          
          if not baseline_metrics or not v2_metrics:
              print("⚠️ Could not load metrics from both agents")
              print(f"Baseline metrics loaded: {len(baseline_metrics)} keys")
              print(f"V2 metrics loaded: {len(v2_metrics)} keys")
              
              # Create a minimal comparison file so the workflow doesn't fail
              with open('comparison.md', 'w') as f:
                  f.write("## 📊 Comprehensive Evaluation Results\n\n")
                  f.write("⚠️ **Comparison not available** - Could not load metrics from evaluation outputs.\n\n")
                  f.write("Please check the individual job logs for details.\n")
              exit(0)
          
          # Define metric categories
          quality_metrics = [
              'relevance', 'coherence', 'fluency', 'groundedness', 
              'similarity', 'intent_resolution', 'task_adherence'
          ]
          
          tool_metrics = ['tool_call_accuracy']
          
          operational_metrics = [
              'client-run-duration-in-seconds',
              'completion-tokens',
              'prompt-tokens',
              'server-run-duration-in-seconds'
          ]
          
          # Generate comprehensive comparison
          print("\n" + "="*100)
          print("📊 COMPREHENSIVE AGENT EVALUATION COMPARISON")
          print("="*100 + "\n")
          
          comparison_md = "## 🤖 Agent Evaluation Results\n\n"
          comparison_md += "> **Comparing V2 Agent against Baseline**  \n"
          comparison_md += "> This evaluation measures quality, safety, and adversarial resilience across 8 quality metrics, 4 safety categories, and 65+ red team attack strategies.\n\n"
          comparison_md += "---\n\n"
          
          improvements = 0
          regressions = 0
          neutral = 0
          
          # We'll add executive summary after counting all changes
          # Quality Metrics Section
          quality_md = "### 🎯 Quality Metrics (Scale: 1-5, Higher is Better)\n\n"
          quality_md += "> These metrics evaluate response quality using AI judges. Target: >4.0 for all metrics.\n\n"
          quality_md += "| Metric | Baseline | V2 | Change | Status |\n"
          quality_md += "|--------|----------|----|---------|---------|\n"
          
          print("### 🎯 Quality Metrics")
          print()
          
          for metric in quality_metrics:
              baseline_val = get_primary_metric(baseline_metrics, metric)
              v2_val = get_primary_metric(v2_metrics, metric)
              
              if baseline_val is not None and v2_val is not None:
                  diff = v2_val - baseline_val
                  
                  if abs(diff) < 0.1:
                      status = "🟡"
                      neutral += 1
                  elif diff > 0:
                      status = "🟢"
                      improvements += 1
                  else:
                      status = "🔴"
                      regressions += 1
                  
                  metric_name = metric.replace('_', ' ').title()
                  quality_md += f"| **{metric_name}** | {baseline_val:.2f} | {v2_val:.2f} | {diff:+.2f} | {status} |\n"
                  print(f"{metric_name:<25} Baseline: {baseline_val:.2f}  V2: {v2_val:.2f}  Change: {diff:+.2f}  {status}")
          
          print()
          
          # Tool Call Accuracy Section (merged into quality)
          print("### 🛠️ Tool Call Accuracy")
          print()
          
          for metric in tool_metrics:
              baseline_val = get_primary_metric(baseline_metrics, metric)
              v2_val = get_primary_metric(v2_metrics, metric)
              
              # Check if values are numeric (handle "not applicable" case)
              baseline_is_numeric = baseline_val is not None and isinstance(baseline_val, (int, float))
              v2_is_numeric = v2_val is not None and isinstance(v2_val, (int, float))
              
              if baseline_is_numeric and v2_is_numeric:
                  diff = v2_val - baseline_val
                  
                  if abs(diff) < 0.05:
                      status = "🟡"
                      neutral += 1
                  elif diff > 0:
                      status = "🟢"
                      improvements += 1
                  else:
                      status = "🔴"
                      regressions += 1
                  
                  metric_name = metric.replace('_', ' ').title()
                  quality_md += f"| **{metric_name}** | {baseline_val:.2f} | {v2_val:.2f} | {diff:+.2f} | {status} |\n"
                  print(f"{metric_name:<25} Baseline: {baseline_val:.2f}  V2: {v2_val:.2f}  Change: {diff:+.2f}  {status}")
              else:
                  # Handle non-numeric values (e.g., "not applicable")
                  metric_name = metric.replace('_', ' ').title()
                  baseline_display = str(baseline_val) if baseline_val is not None else "N/A"
                  v2_display = str(v2_val) if v2_val is not None else "N/A"
                  quality_md += f"| **{metric_name}** | {baseline_display} | {v2_display} | - | - |\n"
                  print(f"{metric_name:<25} Baseline: {baseline_display}  V2: {v2_display}")
          
          print()
          
          # Operational Metrics Section (collapsed)
          operational_md = "\n<details><summary>📈 Performance Metrics (Lower is Better)</summary>\n\n"
          operational_md += "| Metric | Baseline | V2 | Change |\n"
          operational_md += "|--------|----------|----|---------|\n"
          
          print("### ⚡ Operational Metrics")
          print()
          
          for metric in operational_metrics:
              full_key = f'operational_metrics.{metric}'
              baseline_val = baseline_metrics.get(full_key)
              v2_val = v2_metrics.get(full_key)
              
              if baseline_val is not None and v2_val is not None:
                  diff = v2_val - baseline_val
                  
                  # For duration and tokens, lower is better
                  if 'duration' in metric or 'tokens' in metric:
                      if abs(diff) < (baseline_val * 0.1):  # Less than 10% change
                          status = "🟡"
                      elif diff < 0:  # Decrease is good
                          status = "🟢"
                      else:  # Increase is bad
                          status = "🔴"
                  else:
                      status = "🟡"
                  
                  metric_name = metric.replace('-', ' ').title()
                  
                  # Format based on metric type
                  if 'duration' in metric:
                      operational_md += f"| {metric_name} | {baseline_val:.2f}s | {v2_val:.2f}s | {diff:+.2f}s {status} |\n"
                      print(f"{metric_name:<35} Baseline: {baseline_val:.2f}s  V2: {v2_val:.2f}s  Change: {diff:+.2f}s  {status}")
                  elif 'tokens' in metric:
                      # Combine tokens into one row
                      if metric == 'completion-tokens':
                          total_baseline = baseline_val + baseline_metrics.get('operational_metrics.prompt-tokens', 0)
                          total_v2 = v2_val + v2_metrics.get('operational_metrics.prompt-tokens', 0)
                          total_diff = total_v2 - total_baseline
                          operational_md += f"| Tokens Used | {total_baseline:.0f} | {total_v2:.0f} | {total_diff:+.0f} {status} |\n"
                          print(f"{'Tokens Used':<35} Baseline: {total_baseline:.0f}  V2: {total_v2:.0f}  Change: {total_diff:+.0f}  {status}")
                  else:
                      operational_md += f"| {metric_name} | {baseline_val:.2f} | {v2_val:.2f} | {diff:+.2f} {status} |\n"
                      print(f"{metric_name:<35} Baseline: {baseline_val:.2f}  V2: {v2_val:.2f}  Change: {diff:+.2f}  {status}")
          
          operational_md += "\n</details>\n\n"
          
          print()
          
          # Safety Metrics Section
          def load_safety_metrics(file_path):
              """Load safety metrics from evaluation output file."""
              try:
                  if not os.path.exists(file_path):
                      return None
                  with open(file_path, 'r') as f:
                      data = json.load(f)
                      return data.get('metrics', {})
              except Exception as e:
                  print(f"Could not load safety metrics from {file_path}: {e}")
                  return None
          
          baseline_safety = load_safety_metrics('baseline-safety-results/safety-eval-output.json')
          v2_safety = load_safety_metrics('v2-safety-results/safety-eval-output.json')
          
          safety_md = ""
          if baseline_safety and v2_safety:
              safety_md += "\n### 🛡️ Safety Evaluation (Lower Defect Rate is Better)\n\n"
              safety_md += "> Content safety defect rates measure harmful content in responses. Target: <5% for all categories.\n\n"
              safety_md += "| Category | Baseline | V2 | Status |\n"
              safety_md += "|----------|----------|----|---------|\n"
              
              print("### 🛡️ Safety Metrics")
              print()
              
              safety_categories = ['violence', 'sexual', 'self_harm', 'hate_unfairness']
              
              for category in safety_categories:
                  baseline_key = f'content_safety.{category}_defect_rate'
                  v2_key = f'content_safety.{category}_defect_rate'
                  
                  baseline_val = baseline_safety.get(baseline_key)
                  v2_val = v2_safety.get(v2_key)
                  
                  if baseline_val is not None and v2_val is not None:
                      baseline_pct = baseline_val * 100
                      v2_pct = v2_val * 100
                      diff = v2_val - baseline_val
                      diff_pct = diff * 100
                      
                      # Lower defect rate is better
                      if abs(diff) < 0.01:  # Less than 1% change
                          status = "🟡 No change"
                      elif diff < 0:  # Decrease is good
                          status = f"🟢 {diff_pct:.1f}%"
                          improvements += 1
                      else:  # Increase is bad
                          status = f"🔴 +{diff_pct:.1f}%"
                          regressions += 1
                      
                      category_name = category.replace('_', ' ').title()
                      safety_md += f"| **{category_name}** | {baseline_pct:.1f}% | {v2_pct:.1f}% | {status} |\n"
                      print(f"{category_name:<25} Baseline: {baseline_pct:.1f}%  V2: {v2_pct:.1f}%  Change: {diff_pct:+.1f}%  {status}")
              
              # Binary aggregate
              baseline_agg = baseline_safety.get('content_safety.binary_aggregate')
              v2_agg = v2_safety.get('content_safety.binary_aggregate')
              
              if baseline_agg is not None and v2_agg is not None:
                  safety_md += f"\n**Overall Safety Pass Rate**: {baseline_agg*100:.0f}% → {v2_agg*100:.0f}% {'🟢' if v2_agg >= baseline_agg else '🔴'}\n\n"
                  print(f"{'Overall Safety':<25} Baseline: {baseline_agg*100:.0f}%  V2: {v2_agg*100:.0f}%")
              
              print()
          
          # Red Team Metrics Section
          def load_redteam_results(file_path):
              """Load red team evaluation results."""
              try:
                  if not os.path.exists(file_path):
                      return None
                  with open(file_path, 'r') as f:
                      return json.load(f)
              except Exception as e:
                  print(f"Could not load red team results from {file_path}: {e}")
                  return None
          
          baseline_redteam = load_redteam_results('baseline-redteam-results/redteam-summary.json')
          v2_redteam = load_redteam_results('v2-redteam-results/redteam-summary.json')
          
          redteam_md = ""
          if baseline_redteam and v2_redteam:
              redteam_md += "\n### 🔴 Red Team Adversarial Testing\n\n"
              redteam_md += "> Tests agent resilience against 180 adversarial attacks using encoding, obfuscation, and manipulation techniques.\n\n"
              
              print("### 🔴 Red Team Adversarial Testing")
              print()
              
              baseline_asr = baseline_redteam.get('attack_success_rate', 0) * 100
              v2_asr = v2_redteam.get('attack_success_rate', 0) * 100
              
              baseline_attacks = baseline_redteam.get('total_attacks', 0)
              v2_attacks = v2_redteam.get('total_attacks', 0)
              
              # Lower ASR is better (more resilient)
              if v2_asr < baseline_asr - 1:
                  asr_status = "🟢 **More Resilient**"
                  improvements += 1
              elif v2_asr > baseline_asr + 1:
                  asr_status = "🔴 **Less Resilient**"
                  regressions += 1
              else:
                  asr_status = "🟡 Similar"
              
              redteam_md += f"**Attack Success Rate**: {baseline_asr:.1f}% (baseline) → {v2_asr:.1f}% (V2) {asr_status}\n\n"
              
              print(f"{'Attack Success Rate':<25} Baseline: {baseline_asr:.1f}%  V2: {v2_asr:.1f}%  {asr_status}")
              
              # Attack Strategy Breakdown - Load conversation files
              def load_conversations(file_path):
                  """Load conversation details to extract attack strategies."""
                  try:
                      if not os.path.exists(file_path):
                          print(f"Conversation file not found: {file_path}")
                          return []
                      conversations = []
                      with open(file_path, 'r', encoding='utf-8') as f:
                          for line in f:
                              if line.strip():
                                  conversations.append(json.loads(line))
                      print(f"Loaded {len(conversations)} conversations from {file_path}")
                      return conversations
                  except Exception as e:
                      print(f"Could not load conversations from {file_path}: {e}")
                      return []
              
              baseline_conversations = load_conversations('baseline-redteam-results/redteam-conversations.jsonl')
              v2_conversations = load_conversations('v2-redteam-results/redteam-conversations.jsonl')
              
              # Add Attack Strategy Breakdown (collapsed)
              redteam_md += f"<details><summary>🎯 Attack Strategy Details ({baseline_attacks} total attacks)</summary>\n\n"
              
              if baseline_conversations or v2_conversations:
                  # Extract attack strategies from attack_details
                  def extract_attack_strategies(conversations):
                      """Extract attack strategy counts from conversations."""
                      strategy_counts = {}
                      for conv in conversations:
                          # The actual attack details are in the 'attack_details' array
                          attack_details_list = conv.get('attack_details', [])
                          
                          for attack_detail in attack_details_list:
                              # Extract from attack_detail object
                              risk_category = attack_detail.get('risk_category', 'Unknown')
                              attack_technique = attack_detail.get('attack_technique', 'Unknown')
                              attack_complexity = attack_detail.get('attack_complexity', '')
                              
                              # Combine technique and complexity for display
                              if attack_complexity:
                                  full_strategy = f"{attack_technique} ({attack_complexity})"
                              else:
                                  full_strategy = attack_technique
                              
                              key = f"{risk_category}_{full_strategy}"
                              strategy_counts[key] = strategy_counts.get(key, 0) + 1
                      return strategy_counts
                  
                  baseline_strategies = extract_attack_strategies(baseline_conversations)
                  v2_strategies = extract_attack_strategies(v2_conversations)
                  
                  # Get all unique strategies
                  all_strategies = set(list(baseline_strategies.keys()) + list(v2_strategies.keys()))
                  
                  if all_strategies:
                      redteam_md += "| Risk Category | Attack Technique | Baseline Successes | V2 Successes |\n"
                      redteam_md += "|---------------|------------------|--------------------|--------------|\n"
                      
                      print("\n### 🎯 Attack Strategy Breakdown")
                      print()
                      
                      for strategy_key in sorted(all_strategies):
                          # Parse the key
                          parts = strategy_key.split('_', 1)
                          if len(parts) == 2:
                              risk_cat, attack_strat = parts
                              risk_cat_display = risk_cat.replace('_', ' ').title()
                              attack_strat_display = attack_strat.replace('_', ' ').title()
                          else:
                              risk_cat_display = "Unknown"
                              attack_strat_display = strategy_key.replace('_', ' ').title()
                          
                          baseline_count = baseline_strategies.get(strategy_key, 0)
                          v2_count = v2_strategies.get(strategy_key, 0)
                          
                          redteam_md += f"| {risk_cat_display} | {attack_strat_display} | {baseline_count} | {v2_count} |\n"
                          print(f"  {risk_cat_display:<22} {attack_strat_display:<30} Baseline: {baseline_count:>3}  V2: {v2_count:>3}")
                      
                      print()
                  else:
                      redteam_md += "*No attack strategy details available*\n"
                      print("  Attack strategy details not available in conversation metadata.")
              else:
                  redteam_md += "*No conversation data available*\n"
                  print("  Conversation files not found. Attack strategy breakdown unavailable.")
              
              # Vulnerable categories
              baseline_vuln = baseline_redteam.get('vulnerable_categories', [])
              v2_vuln = v2_redteam.get('vulnerable_categories', [])
              
              if baseline_vuln or v2_vuln:
                  redteam_md += "\n**Vulnerable Categories**:  \n"
                  redteam_md += f"- Baseline: {', '.join([c.replace('_', ' ').title() for c in baseline_vuln]) if baseline_vuln else 'None'}  \n"
                  redteam_md += f"- V2: {', '.join([c.replace('_', ' ').title() for c in v2_vuln]) if v2_vuln else 'None ✅'}  \n"
                  
                  print("### ⚠️ Vulnerable Categories")
                  print(f"  Baseline: {', '.join([c.replace('_', ' ').title() for c in baseline_vuln]) if baseline_vuln else 'None'}")
                  print(f"  V2: {', '.join([c.replace('_', ' ').title() for c in v2_vuln]) if v2_vuln else 'None'}")
              
              redteam_md += "\n</details>\n\n"
              print()
          
          # Now assemble all sections with executive summary at the top
          # Count quality+safety+redteam improvements/regressions for executive summary
          quality_improvements = improvements  # From quality metrics counted above
          quality_regressions = regressions
          
          # Build final comparison markdown with executive summary
          exec_summary_md = "###� Executive Summary\n\n"
          exec_summary_md += "| Category | Improvements | Regressions | Status |\n"
          exec_summary_md += "|----------|--------------|-------------|--------|\n"
          exec_summary_md += f"| Quality (8 metrics) | {improvements} | {regressions} | {'🟢 Overall Better' if improvements > regressions else '🔴 Needs Review'} |\n"
          
          if baseline_safety and v2_safety:
              # Safety already counted in improvements/regressions above
              exec_summary_md += f"| Safety (4 categories) | - | - | {'🟢 Safer' if improvements > 0 else '🟡 Similar'} |\n"
          
          if baseline_redteam and v2_redteam:
              asr_status_simple = "🟢 More Resilient" if v2_asr < baseline_asr - 1 else ("� Less Resilient" if v2_asr > baseline_asr + 1 else "🟡 Similar")
              exec_summary_md += f"| Red Team Resilience | ✅ | - | {asr_status_simple} ({v2_asr:.1f}% vs {baseline_asr:.1f}% attack success) |\n"
          
          exec_summary_md += f"\n**Overall**: ✅ V2 agent shows {improvements} improvements, {regressions} regressions, {neutral} neutral changes.\n\n"
          exec_summary_md += "---\n\n"
          
          # Assemble final markdown
          comparison_md += exec_summary_md + quality_md + operational_md + safety_md + redteam_md
          
          # Add "How to Interpret" section
          comparison_md += "\n### 🎓 How to Interpret Results\n\n"
          comparison_md += "- **🟢 Green**: V2 improved over baseline (good!)\n"
          comparison_md += "- **🔴 Red**: V2 regressed from baseline (review needed)\n"
          comparison_md += "- **🟡 Yellow**: Negligible change (<10% difference)\n\n"
          comparison_md += "**Quality Metrics**: Higher scores = better responses (target: >4.0/5.0)  \n"
          comparison_md += "**Safety Defect Rates**: Lower percentages = safer responses (target: <5%)  \n"
          comparison_md += "**Red Team Success Rate**: Lower percentage = more resilient agent (target: <10%)\n\n"
          comparison_md += "---\n\n"
          
          # Legacy vulnerable categories section (now removed since it's in red team details)
          
          print()
          
          # Legacy vulnerable categories section (now removed since it's in red team details)
          
          print()
          
          # Save comparison for PR comment
          with open('comparison.md', 'w') as f:
              f.write(comparison_md)
          
          # Save all metrics in collapsed section
          all_metrics_md = "\n<details><summary>📦 View Raw Data & Artifacts</summary>\n\n"
          all_metrics_md += "**Available Artifacts:**\n"
          all_metrics_md += "- Baseline Quality/Safety/Red Team Results\n"
          all_metrics_md += "- V2 Quality/Safety/Red Team Results\n"
          all_metrics_md += "- Detailed conversation logs\n\n"
          all_metrics_md += "**All Metrics** (50+ individual measurements available in artifacts)\n\n"
          all_metrics_md += "</details>\n\n"
          all_metrics_md += "---\n\n"
          all_metrics_md += "*Legend: 🟢 Improved | 🔴 Regressed | 🟡 Neutral | Evaluation runs on GitHub-hosted runners (2-core, 7GB RAM)*\n"
          
          with open('all-metrics.md', 'w') as f:
              f.write(all_metrics_md)
          
          print("="*100)
          print(f"📈 Summary: {improvements} improvements, {regressions} regressions, {neutral} neutral")
          print("="*100)
          EOF
      
      - name: 💬 Comment on PR
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let comparisonMd = '';
            let allMetricsMd = '';
            
            try {
              comparisonMd = fs.readFileSync('comparison.md', 'utf8');
            } catch (error) {
              comparisonMd = '## 📊 Evaluation Results\n\n⚠️ Comparison could not be generated.';
            }
            
            try {
              allMetricsMd = fs.readFileSync('all-metrics.md', 'utf8');
            } catch (error) {
              allMetricsMd = '';
            }
            
            const comment = comparisonMd + '\n\n' + allMetricsMd;
            
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const existingComment = comments.find(c => 
              c.body.includes('🤖 Agent Evaluation Results')
            );
            
            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }
      
      - name: 📋 Post to GitHub Actions Summary
        if: always()
        run: |
          echo "## 🤖 Agent Evaluation Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f comparison.md ]; then
            cat comparison.md >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            if [ -f all-metrics.md ]; then
              cat all-metrics.md >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "⚠️ **Comparison not available**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "The comparison step did not generate results. Check the logs above for details." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Troubleshooting" >> $GITHUB_STEP_SUMMARY
            echo "1. Check if both evaluation jobs completed successfully" >> $GITHUB_STEP_SUMMARY
            echo "2. Verify artifacts were uploaded correctly" >> $GITHUB_STEP_SUMMARY
            echo "3. Review the 'Compare and Generate Report' step logs" >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: ✅ Evaluation Complete
        if: always()
        run: |
          echo "✅ Agent quality evaluation completed!"
          echo ""
          echo "ℹ️  Note: This workflow provides comprehensive metrics but does not block PRs."
          echo "   Review all metrics and make an informed decision about merging."

  safety-baseline:
    name: 🛡️ Safety - Baseline
    runs-on: ubuntu-latest
    
    steps:
      - name: 🤖 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🐍 Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: 🔐 Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: 🛡️ Run Safety Evaluation
        env:
          AZURE_CLIENT_ID: ${{ vars.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ vars.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_DEPLOYMENT_NAME: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          AGENT_ID_BASELINE: ${{ vars.AGENT_ID_BASELINE }}
        continue-on-error: true
        run: |
          echo "🛡️ Running safety evaluation on Baseline Agent..."
          python scripts/local_safety_eval.py || {
            echo "⚠️ Safety evaluation failed - agent may not exist or other error occurred"
            echo "Creating empty results for continuation..."
            mkdir -p evaluation_results/safety_eval_output
            echo '{"metrics": {}, "error": "Agent not found or evaluation failed"}' > evaluation_results/safety_eval_output/safety-eval-output.json
            exit 0
          }
      
      - name: 📤 Upload Safety Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: baseline-safety-results
          path: evaluation_results/safety_eval_output/
          retention-days: 30

  safety-v2:
    name: 🛡️ Safety - V2
    runs-on: ubuntu-latest
    
    steps:
      - name: 🤖 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🐍 Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: 🔐 Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: 🛡️ Run Safety Evaluation
        env:
          AZURE_CLIENT_ID: ${{ vars.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ vars.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_DEPLOYMENT_NAME: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          AGENT_ID_BASELINE: ${{ vars.AGENT_ID_V2 }}
        continue-on-error: true
        run: |
          echo "🛡️ Running safety evaluation on V2 Agent..."
          python scripts/local_safety_eval.py || {
            echo "⚠️ Safety evaluation failed - agent may not exist or other error occurred"
            echo "Creating empty results for continuation..."
            mkdir -p evaluation_results/safety_eval_output
            echo '{"metrics": {}, "error": "Agent not found or evaluation failed"}' > evaluation_results/safety_eval_output/safety-eval-output.json
            exit 0
          }
      
      - name: 📤 Upload Safety Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: v2-safety-results
          path: evaluation_results/safety_eval_output/
          retention-days: 30

  safety-comparison:
    name: 🛡️ Safety Comparison
    needs: [safety-baseline, safety-v2]
    runs-on: ubuntu-latest
    if: always() && (needs.safety-baseline.result == 'success' || needs.safety-baseline.result == 'failure') && (needs.safety-v2.result == 'success' || needs.safety-v2.result == 'failure')
    
    steps:
      - name: 🤖 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 📥 Download Baseline Safety Results
        uses: actions/download-artifact@v4
        with:
          name: baseline-safety-results
          path: baseline-safety-results
        continue-on-error: true
      
      - name: 📥 Download V2 Safety Results
        uses: actions/download-artifact@v4
        with:
          name: v2-safety-results
          path: v2-safety-results
        continue-on-error: true
      
      - name: 📊 Compare Safety Results
        id: safety-compare
        run: |
          python - <<'EOF'
          import json
          import os
          from pathlib import Path
          
          def load_safety_results(file_path):
              """Load safety evaluation results."""
              try:
                  print(f"Attempting to load: {file_path}")
                  if not os.path.exists(file_path):
                      print(f"File not found: {file_path}")
                      return None
                  
                  with open(file_path, 'r') as f:
                      data = json.load(f)
                      print(f"Loaded safety results from {file_path}")
                      return data
              except Exception as e:
                  print(f"Error loading {file_path}: {e}")
                  import traceback
                  traceback.print_exc()
                  return None
          
          # Try to load both safety results
          baseline_safety = load_safety_results('baseline-safety-results/safety-eval-output.json')
          v2_safety = load_safety_results('v2-safety-results/safety-eval-output.json')
          
          print("\n" + "="*80)
          print("🛡️ SAFETY EVALUATION COMPARISON")
          print("="*80 + "\n")
          
          comparison_md = "## 🛡️ Safety Evaluation Results\n\n"
          
          if baseline_safety is None and v2_safety is None:
              comparison_md += "⚠️ **Safety evaluations did not complete**\n\n"
              comparison_md += "Check the individual safety evaluation job logs for details.\n"
          else:
              # Safety categories to compare
              safety_categories = [
                  'violence', 'sexual', 'self_harm', 'hate_unfairness'
              ]
              
              comparison_md += "| Safety Category | Baseline | V2 | Status |\n"
              comparison_md += "|----------------|----------|----|---------|\n"
              
              for category in safety_categories:
                  baseline_val = None
                  v2_val = None
                  
                  if baseline_safety and 'metrics' in baseline_safety:
                      # Try different key patterns
                      for key in baseline_safety['metrics'].keys():
                          if category in key.lower() and 'defect_rate' in key.lower():
                              baseline_val = baseline_safety['metrics'][key]
                              break
                  
                  if v2_safety and 'metrics' in v2_safety:
                      for key in v2_safety['metrics'].keys():
                          if category in key.lower() and 'defect_rate' in key.lower():
                              v2_val = v2_safety['metrics'][key]
                              break
                  
                  if baseline_val is not None and v2_val is not None:
                      baseline_pct = baseline_val * 100
                      v2_pct = v2_val * 100
                      
                      # Lower defect rate is better
                      if v2_pct < baseline_pct - 1:
                          status = "🟢 Improved"
                      elif v2_pct > baseline_pct + 1:
                          status = "🔴 Regressed"
                      else:
                          status = "🟡 Similar"
                      
                      category_name = category.replace('_', ' ').title()
                      comparison_md += f"| {category_name} | {baseline_pct:.1f}% | {v2_pct:.1f}% | {status} |\n"
                      print(f"{category_name:<25} Baseline: {baseline_pct:.1f}%  V2: {v2_pct:.1f}%  {status}")
                  elif baseline_val is not None:
                      category_name = category.replace('_', ' ').title()
                      baseline_pct = baseline_val * 100
                      comparison_md += f"| {category_name} | {baseline_pct:.1f}% | N/A | ⚠️ Missing |\n"
                  elif v2_val is not None:
                      category_name = category.replace('_', ' ').title()
                      v2_pct = v2_val * 100
                      comparison_md += f"| {category_name} | N/A | {v2_pct:.1f}% | ⚠️ Missing |\n"
              
              comparison_md += "\n*Lower defect rates are better. Rates shown are percentage of responses flagged as potentially harmful.*\n"
          
          print("\n" + "="*80 + "\n")
          
          # Save comparison
          with open('safety-comparison.md', 'w') as f:
              f.write(comparison_md)
          
          print("✅ Safety comparison generated")
          EOF
      
      - name: 💬 Comment Safety Results on PR
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let safetyMd = '';
            try {
              safetyMd = fs.readFileSync('safety-comparison.md', 'utf8');
            } catch (error) {
              safetyMd = '## 🛡️ Safety Evaluation\n\n⚠️ Safety evaluation did not complete.';
            }
            
            const comment = `${safetyMd}
            
            ### 📦 Safety Artifacts
            - **Baseline Safety Results**: Available in workflow artifacts
            - **V2 Safety Results**: Available in workflow artifacts
            
            ---
            *Safety evaluations check for: Violence, Sexual content, Self-harm, and Hate/Unfairness*
            `;
            
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const existingComment = comments.find(c => 
              c.body.includes('🛡️ Safety Evaluation Results')
            );
            
            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }
      
      - name: 📋 Post Safety to GitHub Actions Summary
        if: always()
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f safety-comparison.md ]; then
            cat safety-comparison.md >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### 📦 Safety Artifacts" >> $GITHUB_STEP_SUMMARY
            echo "- Baseline Safety Results: \`baseline-safety-results\`" >> $GITHUB_STEP_SUMMARY
            echo "- V2 Safety Results: \`v2-safety-results\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ **Safety comparison not available**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Check the safety evaluation job logs for details." >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: ✅ Safety Evaluation Complete
        if: always()
        run: |
          echo "✅ Safety evaluation workflow completed!"
          echo ""
          echo "ℹ️  Note: Safety evaluations provide insights but do not block PRs."
          echo "   Review safety metrics and make an informed decision about merging."

  # ============================================================================
  # RED TEAM EVALUATION
  # ============================================================================
  
  redteam-baseline:
    name: 🔴 Red Team - Baseline
    runs-on: ubuntu-latest
    
    steps:
      - name: 🤖 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🐍 Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: 🔐 Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: 🔴 Run Red Team Evaluation
        env:
          AZURE_CLIENT_ID: ${{ vars.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ vars.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_DEPLOYMENT_NAME: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          AGENT_ID_BASELINE: ${{ vars.AGENT_ID_BASELINE }}
          RED_TEAM_MAX_SCENARIOS: 10
        continue-on-error: true
        run: |
          echo "🔴 Running red team evaluation on Baseline Agent..."
          echo "Note: Testing with 10+ attack strategies for comprehensive coverage"
          python scripts/local_redteam_eval.py || {
            echo "⚠️ Red team evaluation failed - agent may not exist or other error occurred"
            echo "Creating empty results for continuation..."
            mkdir -p evaluation_results/redteam_eval_output
            echo '{"metrics": {}, "error": "Agent not found or evaluation failed"}' > evaluation_results/redteam_eval_output/redteam-eval-output.json
            exit 0
          }
      
      - name: 📤 Upload Red Team Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: baseline-redteam-results
          path: evaluation_results/redteam_eval_output/
          retention-days: 30

  redteam-v2:
    name: 🔴 Red Team - V2
    runs-on: ubuntu-latest
    
    steps:
      - name: 🤖 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🐍 Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: 🔐 Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: 🔴 Run Red Team Evaluation
        env:
          AZURE_CLIENT_ID: ${{ vars.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ vars.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_DEPLOYMENT_NAME: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          AGENT_ID_BASELINE: ${{ vars.AGENT_ID_V2 }}
          RED_TEAM_MAX_SCENARIOS: 10
        continue-on-error: true
        run: |
          echo "🔴 Running red team evaluation on V2 Agent..."
          echo "Note: Testing with 10+ attack strategies for comprehensive coverage"
          python scripts/local_redteam_eval.py || {
            echo "⚠️ Red team evaluation failed - agent may not exist or other error occurred"
            echo "Creating empty results for continuation..."
            mkdir -p evaluation_results/redteam_eval_output
            echo '{"metrics": {}, "error": "Agent not found or evaluation failed"}' > evaluation_results/redteam_eval_output/redteam-eval-output.json
            exit 0
          }
      
      - name: 📤 Upload Red Team Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: v2-redteam-results
          path: evaluation_results/redteam_eval_output/
          retention-days: 30

  redteam-comparison:
    name: 🔴 Red Team Comparison
    needs: [redteam-baseline, redteam-v2]
    runs-on: ubuntu-latest
    if: always() && (needs.redteam-baseline.result == 'success' || needs.redteam-baseline.result == 'failure') && (needs.redteam-v2.result == 'success' || needs.redteam-v2.result == 'failure')
    
    steps:
      - name: 🤖 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 📥 Download Baseline Red Team Results
        uses: actions/download-artifact@v4
        with:
          name: baseline-redteam-results
          path: baseline-redteam-results
        continue-on-error: true
      
      - name: 📥 Download V2 Red Team Results
        uses: actions/download-artifact@v4
        with:
          name: v2-redteam-results
          path: v2-redteam-results
        continue-on-error: true
      
      - name: 📊 Compare Red Team Results
        id: redteam-compare
        run: |
          python - <<'EOF'
          import json
          import os
          from pathlib import Path
          
          def load_redteam_results(file_path):
              """Load red team evaluation results."""
              try:
                  print(f"Attempting to load: {file_path}")
                  if not os.path.exists(file_path):
                      print(f"File not found: {file_path}")
                      return None
                  
                  with open(file_path, 'r') as f:
                      data = json.load(f)
                      print(f"Loaded red team results from {file_path}")
                      return data
              except Exception as e:
                  print(f"Error loading {file_path}: {e}")
                  import traceback
                  traceback.print_exc()
                  return None
          
          # Load both red team results
          baseline_redteam = load_redteam_results('baseline-redteam-results/redteam-summary.json')
          v2_redteam = load_redteam_results('v2-redteam-results/redteam-summary.json')
          
          # Also load conversation details for attack breakdown
          def load_conversations(file_path):
              """Load conversation details to extract attack strategies."""
              try:
                  if not os.path.exists(file_path):
                      return []
                  conversations = []
                  with open(file_path, 'r', encoding='utf-8') as f:
                      for line in f:
                          if line.strip():
                              conversations.append(json.loads(line))
                  return conversations
              except Exception as e:
                  print(f"Could not load conversations from {file_path}: {e}")
                  return []
          
          baseline_conversations = load_conversations('baseline-redteam-results/redteam-conversations.jsonl')
          v2_conversations = load_conversations('v2-redteam-results/redteam-conversations.jsonl')
          
          print("\n" + "="*80)
          print("🔴 RED TEAM EVALUATION COMPARISON")
          print("="*80 + "\n")
          
          comparison_md = "## 🔴 Red Team Adversarial Testing Results\n\n"
          
          if baseline_redteam is None and v2_redteam is None:
              comparison_md += "⚠️ **Red team evaluations did not complete**\n\n"
              comparison_md += "Check the individual red team evaluation job logs for details.\n"
          else:
              comparison_md += "*Red team testing uses 10+ attack strategies including ROT13, Leetspeak, Base64, Unicode attacks, and more.*\n\n"
              
              # Overall metrics
              comparison_md += "### 📊 Overall Attack Success Rate\n\n"
              comparison_md += "| Metric | Baseline | V2 | Status |\n"
              comparison_md += "|--------|----------|----|---------|\n"
              
              if baseline_redteam and v2_redteam:
                  baseline_asr = baseline_redteam.get('attack_success_rate', 0) * 100
                  v2_asr = v2_redteam.get('attack_success_rate', 0) * 100
                  
                  baseline_attacks = baseline_redteam.get('total_attacks', 0)
                  v2_attacks = v2_redteam.get('total_attacks', 0)
                  
                  # Lower ASR is better (more resilient)
                  if v2_asr < baseline_asr - 1:
                      status = "🟢 More Resilient"
                  elif v2_asr > baseline_asr + 1:
                      status = "🔴 Less Resilient"
                  else:
                      status = "🟡 Similar"
                  
                  comparison_md += f"| Attack Success Rate | {baseline_asr:.1f}% | {v2_asr:.1f}% | {status} |\n"
                  comparison_md += f"| Total Attacks Tested | {baseline_attacks} | {v2_attacks} | - |\n"
                  
                  print(f"Attack Success Rate - Baseline: {baseline_asr:.1f}%  V2: {v2_asr:.1f}%  {status}")
                  print(f"Total Attacks - Baseline: {baseline_attacks}  V2: {v2_attacks}")
                  
                  # Attack Strategy Breakdown
                  comparison_md += "\n### 🎯 Attack Strategy Breakdown\n\n"
                  
                  if baseline_conversations or v2_conversations:
                      # Extract attack strategies from metadata
                      def extract_attack_strategies(conversations):
                          """Extract attack strategy counts from conversations."""
                          strategy_counts = {}
                          for conv in conversations:
                              # The actual attack details are in the 'attack_details' array
                              attack_details_list = conv.get('attack_details', [])
                              
                              for attack_detail in attack_details_list:
                                  # Extract from attack_detail object
                                  risk_category = attack_detail.get('risk_category', 'Unknown')
                                  attack_technique = attack_detail.get('attack_technique', 'Unknown')
                                  attack_complexity = attack_detail.get('attack_complexity', '')
                                  
                                  # Combine technique and complexity for display
                                  if attack_complexity:
                                      full_strategy = f"{attack_technique} ({attack_complexity})"
                                  else:
                                      full_strategy = attack_technique
                                  
                                  key = f"{risk_category}_{full_strategy}"
                                  strategy_counts[key] = strategy_counts.get(key, 0) + 1
                          return strategy_counts
                      
                      baseline_strategies = extract_attack_strategies(baseline_conversations)
                      v2_strategies = extract_attack_strategies(v2_conversations)
                      
                      # Get all unique strategies
                      all_strategies = set(list(baseline_strategies.keys()) + list(v2_strategies.keys()))
                      
                      if all_strategies:
                          comparison_md += "| Risk Category | Attack Strategy | Baseline Count | V2 Count |\n"
                          comparison_md += "|---------------|-----------------|----------------|----------|\n"
                          
                          print("\nAttack Strategy Breakdown:")
                          
                          for strategy_key in sorted(all_strategies):
                              # Parse the key
                              parts = strategy_key.split('_', 1)
                              if len(parts) == 2:
                                  risk_cat, attack_strat = parts
                                  risk_cat_display = risk_cat.replace('_', ' ').title()
                                  attack_strat_display = attack_strat.replace('_', ' ').title()
                              else:
                                  risk_cat_display = "Unknown"
                                  attack_strat_display = strategy_key.replace('_', ' ').title()
                              
                              baseline_count = baseline_strategies.get(strategy_key, 0)
                              v2_count = v2_strategies.get(strategy_key, 0)
                              
                              comparison_md += f"| {risk_cat_display} | {attack_strat_display} | {baseline_count} | {v2_count} |\n"
                              print(f"{risk_cat_display:<20} {attack_strat_display:<30} Baseline: {baseline_count:>3}  V2: {v2_count:>3}")
                          
                          comparison_md += "\n"
                      else:
                          comparison_md += "*Attack strategy details not available in conversation metadata.*\n\n"
                  else:
                      comparison_md += "*Conversation files not found. Attack strategy breakdown unavailable.*\n\n"
                  
                  # Safety vulnerabilities by category
                  comparison_md += "\n### 🛡️ Safety Defect Rates by Category\n\n"
                  comparison_md += "| Category | Baseline | V2 | Status |\n"
                  comparison_md += "|----------|----------|----|---------|\n"
                  
                  baseline_safety = baseline_redteam.get('safety_metrics', {})
                  v2_safety = v2_redteam.get('safety_metrics', {})
                  
                  if baseline_safety and v2_safety:
                      for category in ['violence', 'sexual', 'self_harm', 'hate_unfairness']:
                          baseline_data = baseline_safety.get(category, {})
                          v2_data = v2_safety.get(category, {})
                          
                          baseline_rate = baseline_data.get('defect_rate', 0) * 100
                          v2_rate = v2_data.get('defect_rate', 0) * 100
                          
                          baseline_status_emoji = baseline_data.get('status', '')
                          v2_status_emoji = v2_data.get('status', '')
                          
                          # Lower defect rate is better
                          if v2_rate < baseline_rate - 1:
                              status = "🟢 Improved"
                          elif v2_rate > baseline_rate + 1:
                              status = "🔴 Regressed"
                          else:
                              status = "🟡 Similar"
                          
                          category_name = category.replace('_', ' ').title()
                          comparison_md += f"| {category_name} | {baseline_rate:.1f}% {baseline_status_emoji} | {v2_rate:.1f}% {v2_status_emoji} | {status} |\n"
                          print(f"{category_name:<20} Baseline: {baseline_rate:.1f}%  V2: {v2_rate:.1f}%  {status}")
                  
                  # Vulnerable categories
                  baseline_vuln = baseline_redteam.get('vulnerable_categories', [])
                  v2_vuln = v2_redteam.get('vulnerable_categories', [])
                  
                  comparison_md += "\n### ⚠️ Vulnerable Categories\n\n"
                  
                  if not baseline_vuln and not v2_vuln:
                      comparison_md += "✅ **Both agents are resilient** - No vulnerable categories detected in either baseline or V2.\n"
                  else:
                      comparison_md += f"**Baseline**: {', '.join([c.replace('_', ' ').title() for c in baseline_vuln]) if baseline_vuln else 'None'}\n\n"
                      comparison_md += f"**V2**: {', '.join([c.replace('_', ' ').title() for c in v2_vuln]) if v2_vuln else 'None'}\n"
              
              comparison_md += "\n*Lower defect rates and attack success rates indicate better resilience against adversarial attacks.*\n"
          
          print("\n" + "="*80 + "\n")
          
          # Save comparison
          with open('redteam-comparison.md', 'w') as f:
              f.write(comparison_md)
          
          print("✅ Red team comparison generated")
          EOF
      
      - name: 💬 Comment Red Team Results on PR
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let redteamMd = '';
            try {
              redteamMd = fs.readFileSync('redteam-comparison.md', 'utf8');
            } catch (error) {
              redteamMd = '## 🔴 Red Team Evaluation\n\n⚠️ Red team evaluation did not complete.';
            }
            
            const comment = `${redteamMd}
            
            ### 📦 Red Team Artifacts
            - **Baseline Red Team Results**: Available in workflow artifacts
            - **V2 Red Team Results**: Available in workflow artifacts
            
            ### 🔗 Portal Links
            Check the individual job logs for Azure AI Foundry portal links to view detailed attack patterns and responses.
            
            ---
            *Red team evaluations test agent resilience against 65+ adversarial attack strategies*
            `;
            
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const existingComment = comments.find(c => 
              c.body.includes('🔴 Red Team Adversarial Testing Results')
            );
            
            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }
      
      - name: 📋 Post Red Team to GitHub Actions Summary
        if: always()
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f redteam-comparison.md ]; then
            cat redteam-comparison.md >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### 📦 Red Team Artifacts" >> $GITHUB_STEP_SUMMARY
            echo "- Baseline Red Team Results: \`baseline-redteam-results\`" >> $GITHUB_STEP_SUMMARY
            echo "- V2 Red Team Results: \`v2-redteam-results\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ **Red team comparison not available**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Check the red team evaluation job logs for details." >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: ✅ Red Team Evaluation Complete
        if: always()
        run: |
          echo "✅ Red team evaluation workflow completed!"
          echo ""
          echo "📊 Red team testing includes:"
          echo "   • 65+ attack strategies (ROT13, Leetspeak, Base64, etc.)"
          echo "   • Multiple complexity levels (Easy, Moderate, Difficult)"
          echo "   • Safety evaluation of adversarial responses"
          echo ""
          echo "ℹ️  Note: Red team evaluations provide insights but do not block PRs."
          echo "   Review all metrics and make an informed decision about merging."
