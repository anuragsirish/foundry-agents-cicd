name: ğŸ¤– Unified Agent Evaluation

on:
  pull_request:
    types: [opened, synchronize, reopened]
    paths:
      - 'agent-setup/**'
      - 'data/**'
      - 'scripts/**'
      - '.github/workflows/agent-evaluation-unified.yml'

permissions:
  id-token: write
  contents: read
  pull-requests: write

jobs:
  evaluate-baseline:
    name: ğŸ¯ Baseline Agent
    runs-on: ubuntu-latest
    
    steps:
      - name: ğŸ¤– Checkout Repository
        uses: actions/checkout@v4
      
      - name: ğŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: ğŸ“¦ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: ğŸ” Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: ğŸ¯ Evaluate Baseline Agent
        env:
          AZURE_CLIENT_ID: ${{ vars.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ vars.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_DEPLOYMENT_NAME: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          AGENT_ID_BASELINE: ${{ vars.AGENT_ID_BASELINE }}
        run: |
          echo "ğŸ¯ Evaluating Baseline Agent..."
          python scripts/local_agent_eval.py
      
      - name: ğŸ“¤ Upload Baseline Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: baseline-results
          path: evaluation_results/agent_eval_output/
          retention-days: 30

  evaluate-v2:
    name: ğŸš€ V2 Agent
    runs-on: ubuntu-latest
    
    steps:
      - name: ğŸ¤– Checkout Repository
        uses: actions/checkout@v4
      
      - name: ğŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: ğŸ“¦ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: ğŸ” Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: ğŸš€ Evaluate V2 Agent
        env:
          AZURE_CLIENT_ID: ${{ vars.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ vars.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_DEPLOYMENT_NAME: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          AGENT_ID_BASELINE: ${{ vars.AGENT_ID_V2 }}
        run: |
          echo "ğŸš€ Evaluating V2 Agent..."
          python scripts/local_agent_eval.py
      
      - name: ğŸ“¤ Upload V2 Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: v2-results
          path: evaluation_results/agent_eval_output/
          retention-days: 30

  compare-and-report:
    name: ğŸ“Š Compare & Report
    needs: [evaluate-baseline, evaluate-v2]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: ğŸ¤– Checkout Repository
        uses: actions/checkout@v4
      
      - name: ğŸ“¥ Download Baseline Results
        uses: actions/download-artifact@v4
        with:
          name: baseline-results
          path: baseline-results
      
      - name: ğŸ“¥ Download V2 Results
        uses: actions/download-artifact@v4
        with:
          name: v2-results
          path: v2-results
      
      - name: ğŸ“Š Compare and Generate Report
        id: compare
        run: |
          python - <<'EOF'
          import json
          import os
          from pathlib import Path
          
          def load_metrics(file_path):
              """Load metrics from evaluation output file."""
              try:
                  with open(file_path, 'r') as f:
                      data = json.load(f)
                      return data.get('metrics', {})
              except Exception as e:
                  print(f"Error loading {file_path}: {e}")
                  return {}
          
          def get_primary_metric(metrics_dict, base_key):
              """Get the primary metric value (without prefixes like gpt_)."""
              # Try keys in order of preference
              for key in [base_key, f'gpt_{base_key}', f'{base_key}.{base_key}']:
                  if key in metrics_dict:
                      return metrics_dict[key]
              return None
          
          # Load both evaluation results
          baseline_metrics = load_metrics('baseline-results/agent-eval-output.json')
          v2_metrics = load_metrics('v2-results/agent-eval-output.json')
          
          if not baseline_metrics or not v2_metrics:
              print("âš ï¸ Could not load metrics from both agents")
              exit(0)
          
          # Define metric categories
          quality_metrics = [
              'relevance', 'coherence', 'fluency', 'groundedness', 
              'similarity', 'intent_resolution', 'task_adherence'
          ]
          
          tool_metrics = ['tool_call_accuracy']
          
          operational_metrics = [
              'client-run-duration-in-seconds',
              'completion-tokens',
              'prompt-tokens',
              'server-run-duration-in-seconds'
          ]
          
          # Generate comprehensive comparison
          print("\n" + "="*100)
          print("ğŸ“Š COMPREHENSIVE AGENT EVALUATION COMPARISON")
          print("="*100 + "\n")
          
          comparison_md = "## ğŸ“Š Comprehensive Evaluation Results\n\n"
          
          # Quality Metrics Section
          comparison_md += "### ğŸ¯ Quality Metrics (1-5 scale)\n\n"
          comparison_md += "| Metric | Baseline | V2 | Change | Status |\n"
          comparison_md += "|--------|----------|----|---------|---------|\n"
          
          improvements = 0
          regressions = 0
          neutral = 0
          
          print("### ğŸ¯ Quality Metrics")
          print()
          
          for metric in quality_metrics:
              baseline_val = get_primary_metric(baseline_metrics, metric)
              v2_val = get_primary_metric(v2_metrics, metric)
              
              if baseline_val is not None and v2_val is not None:
                  diff = v2_val - baseline_val
                  
                  if abs(diff) < 0.1:
                      status = "ğŸŸ¡"
                      neutral += 1
                  elif diff > 0:
                      status = "ğŸŸ¢"
                      improvements += 1
                  else:
                      status = "ğŸ”´"
                      regressions += 1
                  
                  metric_name = metric.replace('_', ' ').title()
                  comparison_md += f"| {metric_name} | {baseline_val:.2f} | {v2_val:.2f} | {diff:+.2f} | {status} |\n"
                  print(f"{metric_name:<25} Baseline: {baseline_val:.2f}  V2: {v2_val:.2f}  Change: {diff:+.2f}  {status}")
          
          print()
          
          # Tool Call Accuracy Section
          comparison_md += "\n### ğŸ› ï¸ Tool Call Accuracy\n\n"
          comparison_md += "| Metric | Baseline | V2 | Change | Status |\n"
          comparison_md += "|--------|----------|----|---------|---------|\n"
          
          print("### ğŸ› ï¸ Tool Call Accuracy")
          print()
          
          for metric in tool_metrics:
              baseline_val = get_primary_metric(baseline_metrics, metric)
              v2_val = get_primary_metric(v2_metrics, metric)
              
              if baseline_val is not None and v2_val is not None:
                  diff = v2_val - baseline_val
                  
                  if abs(diff) < 0.05:
                      status = "ğŸŸ¡"
                      neutral += 1
                  elif diff > 0:
                      status = "ğŸŸ¢"
                      improvements += 1
                  else:
                      status = "ğŸ”´"
                      regressions += 1
                  
                  metric_name = metric.replace('_', ' ').title()
                  comparison_md += f"| {metric_name} | {baseline_val:.2f} | {v2_val:.2f} | {diff:+.2f} | {status} |\n"
                  print(f"{metric_name:<25} Baseline: {baseline_val:.2f}  V2: {v2_val:.2f}  Change: {diff:+.2f}  {status}")
          
          print()
          
          # Operational Metrics Section
          comparison_md += "\n### âš¡ Operational Metrics\n\n"
          comparison_md += "| Metric | Baseline | V2 | Change | Status |\n"
          comparison_md += "|--------|----------|----|---------|---------|\n"
          
          print("### âš¡ Operational Metrics")
          print()
          
          for metric in operational_metrics:
              full_key = f'operational_metrics.{metric}'
              baseline_val = baseline_metrics.get(full_key)
              v2_val = v2_metrics.get(full_key)
              
              if baseline_val is not None and v2_val is not None:
                  diff = v2_val - baseline_val
                  
                  # For duration and tokens, lower is better
                  if 'duration' in metric or 'tokens' in metric:
                      if abs(diff) < (baseline_val * 0.1):  # Less than 10% change
                          status = "ğŸŸ¡"
                      elif diff < 0:  # Decrease is good
                          status = "ğŸŸ¢"
                      else:  # Increase is bad
                          status = "ğŸ”´"
                  else:
                      status = "ğŸŸ¡"
                  
                  metric_name = metric.replace('-', ' ').title()
                  
                  # Format based on metric type
                  if 'duration' in metric:
                      comparison_md += f"| {metric_name} | {baseline_val:.2f}s | {v2_val:.2f}s | {diff:+.2f}s | {status} |\n"
                      print(f"{metric_name:<35} Baseline: {baseline_val:.2f}s  V2: {v2_val:.2f}s  Change: {diff:+.2f}s  {status}")
                  elif 'tokens' in metric:
                      comparison_md += f"| {metric_name} | {baseline_val:.0f} | {v2_val:.0f} | {diff:+.0f} | {status} |\n"
                      print(f"{metric_name:<35} Baseline: {baseline_val:.0f}  V2: {v2_val:.0f}  Change: {diff:+.0f}  {status}")
                  else:
                      comparison_md += f"| {metric_name} | {baseline_val:.2f} | {v2_val:.2f} | {diff:+.2f} | {status} |\n"
                      print(f"{metric_name:<35} Baseline: {baseline_val:.2f}  V2: {v2_val:.2f}  Change: {diff:+.2f}  {status}")
          
          print()
          
          # Binary Aggregates Section (Threshold-based metrics)
          comparison_md += "\n### âœ… Threshold Performance (Pass Rate)\n\n"
          comparison_md += "| Metric | Baseline | V2 | Status |\n"
          comparison_md += "|--------|----------|----|---------|\n"
          
          print("### âœ… Threshold Performance")
          print()
          
          binary_metrics = [k for k in baseline_metrics.keys() if k.endswith('.binary_aggregate')]
          
          for metric_key in sorted(binary_metrics):
              baseline_val = baseline_metrics.get(metric_key)
              v2_val = v2_metrics.get(metric_key)
              
              if baseline_val is not None and v2_val is not None:
                  metric_name = metric_key.replace('.binary_aggregate', '').replace('_', ' ').title()
                  
                  baseline_pct = baseline_val * 100
                  v2_pct = v2_val * 100
                  diff = v2_val - baseline_val
                  
                  if abs(diff) < 0.01:
                      status = "ğŸŸ¡"
                  elif diff > 0:
                      status = "ğŸŸ¢"
                  else:
                      status = "ğŸ”´"
                  
                  comparison_md += f"| {metric_name} | {baseline_pct:.0f}% | {v2_pct:.0f}% | {status} |\n"
                  print(f"{metric_name:<30} Baseline: {baseline_pct:.0f}%  V2: {v2_pct:.0f}%  {status}")
          
          print()
          
          # Summary Section
          comparison_md += "\n### ğŸ“ˆ Summary\n\n"
          comparison_md += f"- ğŸŸ¢ **Improvements**: {improvements}\n"
          comparison_md += f"- ğŸ”´ **Regressions**: {regressions}\n"
          comparison_md += f"- ğŸŸ¡ **Neutral**: {neutral}\n\n"
          
          if regressions == 0:
              comparison_md += "âœ… **No regressions detected** - V2 agent maintains or improves quality!\n"
          elif improvements > regressions:
              comparison_md += f"âš ï¸ **Mixed results** - {improvements} improvements vs {regressions} regressions. Review details before merging.\n"
          else:
              comparison_md += f"âš ï¸ **Quality concerns** - {regressions} regressions detected. Consider investigating before merging.\n"
          
          print("="*100)
          print(f"ğŸ“ˆ Summary: {improvements} improvements, {regressions} regressions, {neutral} neutral")
          print("="*100)
          print()
          
          # Save comparison for PR comment
          with open('comparison.md', 'w') as f:
              f.write(comparison_md)
          
          # Save all metrics for detailed view
          all_metrics_md = "\n### ğŸ“‹ All Baseline Metrics\n\n"
          all_metrics_md += "<details><summary>Click to expand</summary>\n\n"
          all_metrics_md += "| Metric | Value |\n"
          all_metrics_md += "|--------|-------|\n"
          
          for key in sorted(baseline_metrics.keys()):
              val = baseline_metrics[key]
              if isinstance(val, (int, float)):
                  all_metrics_md += f"| `{key}` | {val:.4f} |\n"
          
          all_metrics_md += "\n</details>\n\n"
          
          all_metrics_md += "### ğŸ“‹ All V2 Metrics\n\n"
          all_metrics_md += "<details><summary>Click to expand</summary>\n\n"
          all_metrics_md += "| Metric | Value |\n"
          all_metrics_md += "|--------|-------|\n"
          
          for key in sorted(v2_metrics.keys()):
              val = v2_metrics[key]
              if isinstance(val, (int, float)):
                  all_metrics_md += f"| `{key}` | {val:.4f} |\n"
          
          all_metrics_md += "\n</details>\n"
          
          with open('all-metrics.md', 'w') as f:
              f.write(all_metrics_md)
          
          print("âœ… Comparison generated successfully!")
          EOF
      
      - name: ğŸ’¬ Comment on PR
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let comparisonMd = '';
            let allMetricsMd = '';
            
            try {
              comparisonMd = fs.readFileSync('comparison.md', 'utf8');
            } catch (error) {
              comparisonMd = '## ğŸ“Š Evaluation Results\n\nâš ï¸ Comparison could not be generated.';
            }
            
            try {
              allMetricsMd = fs.readFileSync('all-metrics.md', 'utf8');
            } catch (error) {
              allMetricsMd = '';
            }
            
            const artifactsSection = `
            ### ğŸ“¦ Artifacts
            - **Baseline Results**: Download from workflow artifacts
            - **V2 Results**: Download from workflow artifacts
            
            ---
            *Legend: ğŸŸ¢ Improved | ğŸ”´ Regressed | ğŸŸ¡ Neutral*
            `;
            
            const comment = comparisonMd + '\n\n' + allMetricsMd + '\n\n' + artifactsSection;
            
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const existingComment = comments.find(c => 
              c.body.includes('ğŸ“Š Comprehensive Evaluation Results')
            );
            
            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }
      
      - name: ğŸ“‹ Post to GitHub Actions Summary
        if: always()
        run: |
          if [ -f comparison.md ]; then
            cat comparison.md >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            if [ -f all-metrics.md ]; then
              cat all-metrics.md >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "âš ï¸ Comparison not available" >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: âœ… Evaluation Complete
        if: always()
        run: |
          echo "âœ… Unified evaluation workflow completed!"
          echo ""
          echo "â„¹ï¸  Note: This workflow provides comprehensive metrics but does not block PRs."
          echo "   Review all metrics and make an informed decision about merging."
