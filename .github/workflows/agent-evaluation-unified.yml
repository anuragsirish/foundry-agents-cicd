name: ğŸ¤– Unified Agent Evaluation (Quality + Safety)

on:
  pull_request:
    types: [opened, synchronize, reopened]
    paths:
      - 'agent-setup/**'
      - 'data/**'
      - 'scripts/**'
      - '.github/workflows/agent-evaluation-unified.yml'

permissions:
  id-token: write
  contents: read
  pull-requests: write

jobs:
  evaluate-baseline:
    name: ğŸ¯ Baseline Agent
    runs-on: ubuntu-latest
    
    steps:
      - name: ğŸ¤– Checkout Repository
        uses: actions/checkout@v4
      
      - name: ğŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: ğŸ“¦ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: ğŸ” Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: ğŸ¯ Evaluate Baseline Agent
        env:
          AZURE_CLIENT_ID: ${{ vars.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ vars.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_DEPLOYMENT_NAME: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          AGENT_ID_BASELINE: ${{ vars.AGENT_ID_BASELINE }}
        run: |
          echo "ğŸ¯ Evaluating Baseline Agent..."
          python scripts/local_agent_eval.py
      
      - name: ğŸ“¤ Upload Baseline Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: baseline-results
          path: evaluation_results/agent_eval_output/
          retention-days: 30

  evaluate-v2:
    name: ğŸš€ V2 Agent
    runs-on: ubuntu-latest
    
    steps:
      - name: ğŸ¤– Checkout Repository
        uses: actions/checkout@v4
      
      - name: ğŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: ğŸ“¦ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: ğŸ” Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: ğŸš€ Evaluate V2 Agent
        env:
          AZURE_CLIENT_ID: ${{ vars.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ vars.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_DEPLOYMENT_NAME: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          AGENT_ID_BASELINE: ${{ vars.AGENT_ID_V2 }}
        run: |
          echo "ğŸš€ Evaluating V2 Agent..."
          python scripts/local_agent_eval.py
      
      - name: ğŸ“¤ Upload V2 Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: v2-results
          path: evaluation_results/agent_eval_output/
          retention-days: 30

  compare-and-report:
    name: ğŸ“Š Compare & Report
    needs: [evaluate-baseline, evaluate-v2]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: ğŸ¤– Checkout Repository
        uses: actions/checkout@v4
      
      - name: ğŸ“¥ Download Baseline Results
        uses: actions/download-artifact@v4
        with:
          name: baseline-results
          path: baseline-results
      
      - name: ğŸ“¥ Download V2 Results
        uses: actions/download-artifact@v4
        with:
          name: v2-results
          path: v2-results
      
      - name: ğŸ“Š Compare and Generate Report
        id: compare
        run: |
          python - <<'EOF'
          import json
          import os
          from pathlib import Path
          
          def load_metrics(file_path):
              """Load metrics from evaluation output file."""
              try:
                  print(f"Attempting to load: {file_path}")
                  if not os.path.exists(file_path):
                      print(f"File not found: {file_path}")
                      # List directory contents for debugging
                      dir_path = os.path.dirname(file_path)
                      if os.path.exists(dir_path):
                          print(f"Contents of {dir_path}:")
                          for item in os.listdir(dir_path):
                              print(f"  - {item}")
                      return {}
                  
                  with open(file_path, 'r') as f:
                      data = json.load(f)
                      metrics = data.get('metrics', {})
                      print(f"Loaded {len(metrics)} metrics from {file_path}")
                      return metrics
              except Exception as e:
                  print(f"Error loading {file_path}: {e}")
                  import traceback
                  traceback.print_exc()
                  return {}
          
          def get_primary_metric(metrics_dict, base_key):
              """Get the primary metric value (without prefixes like gpt_)."""
              # Try keys in order of preference
              for key in [base_key, f'gpt_{base_key}', f'{base_key}.{base_key}']:
                  if key in metrics_dict:
                      return metrics_dict[key]
              return None
          
          # Load both evaluation results
          baseline_metrics = load_metrics('baseline-results/eval-output.json')
          v2_metrics = load_metrics('v2-results/eval-output.json')
          
          if not baseline_metrics or not v2_metrics:
              print("âš ï¸ Could not load metrics from both agents")
              print(f"Baseline metrics loaded: {len(baseline_metrics)} keys")
              print(f"V2 metrics loaded: {len(v2_metrics)} keys")
              
              # Create a minimal comparison file so the workflow doesn't fail
              with open('comparison.md', 'w') as f:
                  f.write("## ğŸ“Š Comprehensive Evaluation Results\n\n")
                  f.write("âš ï¸ **Comparison not available** - Could not load metrics from evaluation outputs.\n\n")
                  f.write("Please check the individual job logs for details.\n")
              exit(0)
          
          # Define metric categories
          quality_metrics = [
              'relevance', 'coherence', 'fluency', 'groundedness', 
              'similarity', 'intent_resolution', 'task_adherence'
          ]
          
          tool_metrics = ['tool_call_accuracy']
          
          operational_metrics = [
              'client-run-duration-in-seconds',
              'completion-tokens',
              'prompt-tokens',
              'server-run-duration-in-seconds'
          ]
          
          # Generate comprehensive comparison
          print("\n" + "="*100)
          print("ğŸ“Š COMPREHENSIVE AGENT EVALUATION COMPARISON")
          print("="*100 + "\n")
          
          comparison_md = "## ğŸ“Š Comprehensive Evaluation Results\n\n"
          
          # Quality Metrics Section
          comparison_md += "### ğŸ¯ Quality Metrics (1-5 scale)\n\n"
          comparison_md += "| Metric | Baseline | V2 | Change | Status |\n"
          comparison_md += "|--------|----------|----|---------|---------|\n"
          
          improvements = 0
          regressions = 0
          neutral = 0
          
          print("### ğŸ¯ Quality Metrics")
          print()
          
          for metric in quality_metrics:
              baseline_val = get_primary_metric(baseline_metrics, metric)
              v2_val = get_primary_metric(v2_metrics, metric)
              
              if baseline_val is not None and v2_val is not None:
                  diff = v2_val - baseline_val
                  
                  if abs(diff) < 0.1:
                      status = "ğŸŸ¡"
                      neutral += 1
                  elif diff > 0:
                      status = "ğŸŸ¢"
                      improvements += 1
                  else:
                      status = "ğŸ”´"
                      regressions += 1
                  
                  metric_name = metric.replace('_', ' ').title()
                  comparison_md += f"| {metric_name} | {baseline_val:.2f} | {v2_val:.2f} | {diff:+.2f} | {status} |\n"
                  print(f"{metric_name:<25} Baseline: {baseline_val:.2f}  V2: {v2_val:.2f}  Change: {diff:+.2f}  {status}")
          
          print()
          
          # Tool Call Accuracy Section
          comparison_md += "\n### ğŸ› ï¸ Tool Call Accuracy\n\n"
          comparison_md += "| Metric | Baseline | V2 | Change | Status |\n"
          comparison_md += "|--------|----------|----|---------|---------|\n"
          
          print("### ğŸ› ï¸ Tool Call Accuracy")
          print()
          
          for metric in tool_metrics:
              baseline_val = get_primary_metric(baseline_metrics, metric)
              v2_val = get_primary_metric(v2_metrics, metric)
              
              if baseline_val is not None and v2_val is not None:
                  diff = v2_val - baseline_val
                  
                  if abs(diff) < 0.05:
                      status = "ğŸŸ¡"
                      neutral += 1
                  elif diff > 0:
                      status = "ğŸŸ¢"
                      improvements += 1
                  else:
                      status = "ğŸ”´"
                      regressions += 1
                  
                  metric_name = metric.replace('_', ' ').title()
                  comparison_md += f"| {metric_name} | {baseline_val:.2f} | {v2_val:.2f} | {diff:+.2f} | {status} |\n"
                  print(f"{metric_name:<25} Baseline: {baseline_val:.2f}  V2: {v2_val:.2f}  Change: {diff:+.2f}  {status}")
          
          print()
          
          # Operational Metrics Section
          comparison_md += "\n### âš¡ Operational Metrics\n\n"
          comparison_md += "| Metric | Baseline | V2 | Change | Status |\n"
          comparison_md += "|--------|----------|----|---------|---------|\n"
          
          print("### âš¡ Operational Metrics")
          print()
          
          for metric in operational_metrics:
              full_key = f'operational_metrics.{metric}'
              baseline_val = baseline_metrics.get(full_key)
              v2_val = v2_metrics.get(full_key)
              
              if baseline_val is not None and v2_val is not None:
                  diff = v2_val - baseline_val
                  
                  # For duration and tokens, lower is better
                  if 'duration' in metric or 'tokens' in metric:
                      if abs(diff) < (baseline_val * 0.1):  # Less than 10% change
                          status = "ğŸŸ¡"
                      elif diff < 0:  # Decrease is good
                          status = "ğŸŸ¢"
                      else:  # Increase is bad
                          status = "ğŸ”´"
                  else:
                      status = "ğŸŸ¡"
                  
                  metric_name = metric.replace('-', ' ').title()
                  
                  # Format based on metric type
                  if 'duration' in metric:
                      comparison_md += f"| {metric_name} | {baseline_val:.2f}s | {v2_val:.2f}s | {diff:+.2f}s | {status} |\n"
                      print(f"{metric_name:<35} Baseline: {baseline_val:.2f}s  V2: {v2_val:.2f}s  Change: {diff:+.2f}s  {status}")
                  elif 'tokens' in metric:
                      comparison_md += f"| {metric_name} | {baseline_val:.0f} | {v2_val:.0f} | {diff:+.0f} | {status} |\n"
                      print(f"{metric_name:<35} Baseline: {baseline_val:.0f}  V2: {v2_val:.0f}  Change: {diff:+.0f}  {status}")
                  else:
                      comparison_md += f"| {metric_name} | {baseline_val:.2f} | {v2_val:.2f} | {diff:+.2f} | {status} |\n"
                      print(f"{metric_name:<35} Baseline: {baseline_val:.2f}  V2: {v2_val:.2f}  Change: {diff:+.2f}  {status}")
          
          print()
          
          # Binary Aggregates Section (Threshold-based metrics)
          comparison_md += "\n### âœ… Threshold Performance (Pass Rate)\n\n"
          comparison_md += "| Metric | Baseline | V2 | Status |\n"
          comparison_md += "|--------|----------|----|---------|\n"
          
          print("### âœ… Threshold Performance")
          print()
          
          binary_metrics = [k for k in baseline_metrics.keys() if k.endswith('.binary_aggregate')]
          
          for metric_key in sorted(binary_metrics):
              baseline_val = baseline_metrics.get(metric_key)
              v2_val = v2_metrics.get(metric_key)
              
              if baseline_val is not None and v2_val is not None:
                  metric_name = metric_key.replace('.binary_aggregate', '').replace('_', ' ').title()
                  
                  baseline_pct = baseline_val * 100
                  v2_pct = v2_val * 100
                  diff = v2_val - baseline_val
                  
                  if abs(diff) < 0.01:
                      status = "ğŸŸ¡"
                  elif diff > 0:
                      status = "ğŸŸ¢"
                  else:
                      status = "ğŸ”´"
                  
                  comparison_md += f"| {metric_name} | {baseline_pct:.0f}% | {v2_pct:.0f}% | {status} |\n"
                  print(f"{metric_name:<30} Baseline: {baseline_pct:.0f}%  V2: {v2_pct:.0f}%  {status}")
          
          print()
          
          # Summary Section
          comparison_md += "\n### ğŸ“ˆ Summary\n\n"
          comparison_md += f"- ğŸŸ¢ **Improvements**: {improvements}\n"
          comparison_md += f"- ğŸ”´ **Regressions**: {regressions}\n"
          comparison_md += f"- ğŸŸ¡ **Neutral**: {neutral}\n\n"
          
          if regressions == 0:
              comparison_md += "âœ… **No regressions detected** - V2 agent maintains or improves quality!\n"
          elif improvements > regressions:
              comparison_md += f"âš ï¸ **Mixed results** - {improvements} improvements vs {regressions} regressions. Review details before merging.\n"
          else:
              comparison_md += f"âš ï¸ **Quality concerns** - {regressions} regressions detected. Consider investigating before merging.\n"
          
          print("="*100)
          print(f"ğŸ“ˆ Summary: {improvements} improvements, {regressions} regressions, {neutral} neutral")
          print("="*100)
          print()
          
          # Save comparison for PR comment
          with open('comparison.md', 'w') as f:
              f.write(comparison_md)
          
          # Save all metrics for detailed view
          all_metrics_md = "\n### ğŸ“‹ All Baseline Metrics\n\n"
          all_metrics_md += "<details><summary>Click to expand</summary>\n\n"
          all_metrics_md += "| Metric | Value |\n"
          all_metrics_md += "|--------|-------|\n"
          
          for key in sorted(baseline_metrics.keys()):
              val = baseline_metrics[key]
              if isinstance(val, (int, float)):
                  all_metrics_md += f"| `{key}` | {val:.4f} |\n"
          
          all_metrics_md += "\n</details>\n\n"
          
          all_metrics_md += "### ğŸ“‹ All V2 Metrics\n\n"
          all_metrics_md += "<details><summary>Click to expand</summary>\n\n"
          all_metrics_md += "| Metric | Value |\n"
          all_metrics_md += "|--------|-------|\n"
          
          for key in sorted(v2_metrics.keys()):
              val = v2_metrics[key]
              if isinstance(val, (int, float)):
                  all_metrics_md += f"| `{key}` | {val:.4f} |\n"
          
          all_metrics_md += "\n</details>\n"
          
          with open('all-metrics.md', 'w') as f:
              f.write(all_metrics_md)
          
          print("âœ… Comparison generated successfully!")
          EOF
      
      - name: ğŸ’¬ Comment on PR
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let comparisonMd = '';
            let allMetricsMd = '';
            
            try {
              comparisonMd = fs.readFileSync('comparison.md', 'utf8');
            } catch (error) {
              comparisonMd = '## ğŸ“Š Evaluation Results\n\nâš ï¸ Comparison could not be generated.';
            }
            
            try {
              allMetricsMd = fs.readFileSync('all-metrics.md', 'utf8');
            } catch (error) {
              allMetricsMd = '';
            }
            
            const artifactsSection = `
            ### ğŸ“¦ Artifacts
            - **Baseline Results**: Download from workflow artifacts
            - **V2 Results**: Download from workflow artifacts
            
            ---
            *Legend: ğŸŸ¢ Improved | ğŸ”´ Regressed | ğŸŸ¡ Neutral*
            `;
            
            const comment = comparisonMd + '\n\n' + allMetricsMd + '\n\n' + artifactsSection;
            
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const existingComment = comments.find(c => 
              c.body.includes('ğŸ“Š Comprehensive Evaluation Results')
            );
            
            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }
      
      - name: ğŸ“‹ Post to GitHub Actions Summary
        if: always()
        run: |
          echo "## ğŸ¤– Unified Agent Evaluation" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f comparison.md ]; then
            cat comparison.md >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            if [ -f all-metrics.md ]; then
              cat all-metrics.md >> $GITHUB_STEP_SUMMARY
            fi
            
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ğŸ“¦ Artifacts" >> $GITHUB_STEP_SUMMARY
            echo "- Baseline Results: \`baseline-results\`" >> $GITHUB_STEP_SUMMARY
            echo "- V2 Results: \`v2-results\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ **Comparison not available**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "The comparison step did not generate results. Check the logs above for details." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Troubleshooting" >> $GITHUB_STEP_SUMMARY
            echo "1. Check if both evaluation jobs completed successfully" >> $GITHUB_STEP_SUMMARY
            echo "2. Verify artifacts were uploaded correctly" >> $GITHUB_STEP_SUMMARY
            echo "3. Review the 'Compare and Generate Report' step logs" >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: âœ… Evaluation Complete
        if: always()
        run: |
          echo "âœ… Agent quality evaluation completed!"
          echo ""
          echo "â„¹ï¸  Note: This workflow provides comprehensive metrics but does not block PRs."
          echo "   Review all metrics and make an informed decision about merging."

  safety-baseline:
    name: ğŸ›¡ï¸ Safety - Baseline
    runs-on: ubuntu-latest
    
    steps:
      - name: ğŸ¤– Checkout Repository
        uses: actions/checkout@v4
      
      - name: ğŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: ğŸ“¦ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: ğŸ” Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: ğŸ›¡ï¸ Run Safety Evaluation
        env:
          AZURE_CLIENT_ID: ${{ vars.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ vars.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_DEPLOYMENT_NAME: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          AGENT_ID_BASELINE: ${{ vars.AGENT_ID_BASELINE }}
        run: |
          echo "ğŸ›¡ï¸ Running safety evaluation on Baseline Agent..."
          python scripts/local_safety_eval.py
      
      - name: ğŸ“¤ Upload Safety Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: baseline-safety-results
          path: evaluation_results/safety_eval_output/
          retention-days: 30

  safety-v2:
    name: ğŸ›¡ï¸ Safety - V2
    runs-on: ubuntu-latest
    
    steps:
      - name: ğŸ¤– Checkout Repository
        uses: actions/checkout@v4
      
      - name: ğŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: ğŸ“¦ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: ğŸ” Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: ğŸ›¡ï¸ Run Safety Evaluation
        env:
          AZURE_CLIENT_ID: ${{ vars.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ vars.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_DEPLOYMENT_NAME: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          AGENT_ID_BASELINE: ${{ vars.AGENT_ID_V2 }}
        run: |
          echo "ğŸ›¡ï¸ Running safety evaluation on V2 Agent..."
          python scripts/local_safety_eval.py
      
      - name: ğŸ“¤ Upload Safety Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: v2-safety-results
          path: evaluation_results/safety_eval_output/
          retention-days: 30

  safety-comparison:
    name: ğŸ›¡ï¸ Safety Comparison
    needs: [safety-baseline, safety-v2]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: ğŸ¤– Checkout Repository
        uses: actions/checkout@v4
      
      - name: ğŸ“¥ Download Baseline Safety Results
        uses: actions/download-artifact@v4
        with:
          name: baseline-safety-results
          path: baseline-safety-results
        continue-on-error: true
      
      - name: ğŸ“¥ Download V2 Safety Results
        uses: actions/download-artifact@v4
        with:
          name: v2-safety-results
          path: v2-safety-results
        continue-on-error: true
      
      - name: ğŸ“Š Compare Safety Results
        id: safety-compare
        run: |
          python - <<'EOF'
          import json
          import os
          from pathlib import Path
          
          def load_safety_results(file_path):
              """Load safety evaluation results."""
              try:
                  print(f"Attempting to load: {file_path}")
                  if not os.path.exists(file_path):
                      print(f"File not found: {file_path}")
                      return None
                  
                  with open(file_path, 'r') as f:
                      data = json.load(f)
                      print(f"Loaded safety results from {file_path}")
                      return data
              except Exception as e:
                  print(f"Error loading {file_path}: {e}")
                  import traceback
                  traceback.print_exc()
                  return None
          
          # Try to load both safety results
          baseline_safety = load_safety_results('baseline-safety-results/safety-eval-output.json')
          v2_safety = load_safety_results('v2-safety-results/safety-eval-output.json')
          
          print("\n" + "="*80)
          print("ğŸ›¡ï¸ SAFETY EVALUATION COMPARISON")
          print("="*80 + "\n")
          
          comparison_md = "## ğŸ›¡ï¸ Safety Evaluation Results\n\n"
          
          if baseline_safety is None and v2_safety is None:
              comparison_md += "âš ï¸ **Safety evaluations did not complete**\n\n"
              comparison_md += "Check the individual safety evaluation job logs for details.\n"
          else:
              # Safety categories to compare
              safety_categories = [
                  'violence', 'sexual', 'self_harm', 'hate_unfairness'
              ]
              
              comparison_md += "| Safety Category | Baseline | V2 | Status |\n"
              comparison_md += "|----------------|----------|----|---------|\n"
              
              for category in safety_categories:
                  baseline_val = None
                  v2_val = None
                  
                  if baseline_safety and 'metrics' in baseline_safety:
                      # Try different key patterns
                      for key in baseline_safety['metrics'].keys():
                          if category in key.lower() and 'defect_rate' in key.lower():
                              baseline_val = baseline_safety['metrics'][key]
                              break
                  
                  if v2_safety and 'metrics' in v2_safety:
                      for key in v2_safety['metrics'].keys():
                          if category in key.lower() and 'defect_rate' in key.lower():
                              v2_val = v2_safety['metrics'][key]
                              break
                  
                  if baseline_val is not None and v2_val is not None:
                      baseline_pct = baseline_val * 100
                      v2_pct = v2_val * 100
                      
                      # Lower defect rate is better
                      if v2_pct < baseline_pct - 1:
                          status = "ğŸŸ¢ Improved"
                      elif v2_pct > baseline_pct + 1:
                          status = "ğŸ”´ Regressed"
                      else:
                          status = "ğŸŸ¡ Similar"
                      
                      category_name = category.replace('_', ' ').title()
                      comparison_md += f"| {category_name} | {baseline_pct:.1f}% | {v2_pct:.1f}% | {status} |\n"
                      print(f"{category_name:<25} Baseline: {baseline_pct:.1f}%  V2: {v2_pct:.1f}%  {status}")
                  elif baseline_val is not None:
                      category_name = category.replace('_', ' ').title()
                      baseline_pct = baseline_val * 100
                      comparison_md += f"| {category_name} | {baseline_pct:.1f}% | N/A | âš ï¸ Missing |\n"
                  elif v2_val is not None:
                      category_name = category.replace('_', ' ').title()
                      v2_pct = v2_val * 100
                      comparison_md += f"| {category_name} | N/A | {v2_pct:.1f}% | âš ï¸ Missing |\n"
              
              comparison_md += "\n*Lower defect rates are better. Rates shown are percentage of responses flagged as potentially harmful.*\n"
          
          print("\n" + "="*80 + "\n")
          
          # Save comparison
          with open('safety-comparison.md', 'w') as f:
              f.write(comparison_md)
          
          print("âœ… Safety comparison generated")
          EOF
      
      - name: ğŸ’¬ Comment Safety Results on PR
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let safetyMd = '';
            try {
              safetyMd = fs.readFileSync('safety-comparison.md', 'utf8');
            } catch (error) {
              safetyMd = '## ğŸ›¡ï¸ Safety Evaluation\n\nâš ï¸ Safety evaluation did not complete.';
            }
            
            const comment = `${safetyMd}
            
            ### ğŸ“¦ Safety Artifacts
            - **Baseline Safety Results**: Available in workflow artifacts
            - **V2 Safety Results**: Available in workflow artifacts
            
            ---
            *Safety evaluations check for: Violence, Sexual content, Self-harm, and Hate/Unfairness*
            `;
            
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const existingComment = comments.find(c => 
              c.body.includes('ğŸ›¡ï¸ Safety Evaluation Results')
            );
            
            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }
      
      - name: ğŸ“‹ Post Safety to GitHub Actions Summary
        if: always()
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f safety-comparison.md ]; then
            cat safety-comparison.md >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ğŸ“¦ Safety Artifacts" >> $GITHUB_STEP_SUMMARY
            echo "- Baseline Safety Results: \`baseline-safety-results\`" >> $GITHUB_STEP_SUMMARY
            echo "- V2 Safety Results: \`v2-safety-results\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ **Safety comparison not available**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Check the safety evaluation job logs for details." >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: âœ… Safety Evaluation Complete
        if: always()
        run: |
          echo "âœ… Safety evaluation workflow completed!"
          echo ""
          echo "â„¹ï¸  Note: Safety evaluations provide insights but do not block PRs."
          echo "   Review safety metrics and make an informed decision about merging."
