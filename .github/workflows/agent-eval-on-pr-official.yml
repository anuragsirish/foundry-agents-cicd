name: "Agent Evaluation on Pull Request (Official Action) - DISABLED"

on:
  # Disabled - using custom workflow instead
  # pull_request:
  #   branches:
  #     - main
  #   paths:
  #     - 'agent-setup/**'
  #     - 'data/agent-eval-data.json'
  #     - '.github/workflows/agent-eval-on-pr-official.yml'
  
  workflow_dispatch:
    inputs:
      agent_ids:
        description: 'Comma-separated agent IDs to evaluate'
        required: false
        type: string

permissions:
  id-token: write
  contents: read
  pull-requests: write

jobs:
  run-agent-evaluation:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout PR Branch
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history to access main branch
      
      - name: Azure Login using Federated Credentials
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
      
      - name: Check for Baseline Results
        id: check-baseline
        run: |
          # Check if baseline results exist on main branch
          git fetch origin main
          
          # Check if baseline file exists
          if git show origin/main:evaluation_results/baseline/baseline_metrics.json > /dev/null 2>&1; then
            echo "baseline_exists=true" >> $GITHUB_OUTPUT
            echo "üìä Baseline found on main branch"
            
            # Extract baseline to temp location
            git show origin/main:evaluation_results/baseline/baseline_metrics.json > /tmp/baseline_metrics.json
          else
            echo "baseline_exists=false" >> $GITHUB_OUTPUT
            echo "üìù No baseline found - this will be the first run"
          fi
      
      - name: Set Agent IDs for Comparison
        id: set-agents
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ] && [ -n "${{ github.event.inputs.agent_ids }}" ]; then
            echo "agent_ids=${{ github.event.inputs.agent_ids }}" >> $GITHUB_OUTPUT
          else
            # Evaluate both agents - official action handles comparison automatically
            echo "agent_ids=${{ vars.AGENT_ID_BASELINE }},${{ vars.AGENT_ID_V2 }}" >> $GITHUB_OUTPUT
          fi
          
          echo "ü§ñ Agents to evaluate and compare: $(cat $GITHUB_OUTPUT | grep agent_ids | cut -d'=' -f2)"
      
      - name: Display Configuration
        run: |
          echo "üîç Evaluation Configuration:"
          echo "   Agent IDs: ${{ steps.set-agents.outputs.agent_ids }}"
          echo "   Baseline (first): ${{ vars.AGENT_ID_BASELINE }}"
          echo "   Variant (second): ${{ vars.AGENT_ID_V2 }}"
          echo "   Project Endpoint: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}"
          echo "   Deployment: ${{ vars.AZURE_DEPLOYMENT_NAME }}"
          echo "   API Version: ${{ vars.API_VERSION }}"
          echo "   Data Path: ${{ github.workspace }}/data/agent-eval-data.json"
      
      - name: Run AI Agent Evaluation with Comparison
        id: run-evaluation
        uses: microsoft/ai-agent-evals@v2-beta
        with:
          azure-ai-project-endpoint: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          deployment-name: ${{ vars.AZURE_DEPLOYMENT_NAME }}
          agent-ids: ${{ steps.set-agents.outputs.agent_ids }}
          baseline-agent-id: ${{ vars.AGENT_ID_BASELINE }}
          data-path: ${{ github.workspace }}/data/agent-eval-data.json
          evaluation-result-view: 'all-scores'
          api-version: ${{ vars.API_VERSION }}
        env:
          AZURE_OPENAI_ENDPOINT: https://nielsen-agent-demo-resource.openai.azure.com/
          AZURE_OPENAI_API_VERSION: ${{ vars.API_VERSION }}
      
      - name: Extract and Display All Metrics
        id: parse-results
        if: success()
        run: |
          python3 << 'EOF'
          import json
          import os
          import glob
          
          # Find the evaluation output file created by the official action
          eval_files = glob.glob('data/eval-output_*.json')
          
          if not eval_files:
              print("‚ö†Ô∏è No evaluation output file found")
              exit(0)
          
          eval_file = eval_files[0]
          print(f"üìÑ Reading evaluation results from: {eval_file}")
          
          # Read evaluation results
          with open(eval_file, 'r') as f:
              results = json.load(f)
          
          metrics = results.get('metrics', {})
          
          # Extract comprehensive metrics
          extracted = {
              # AI Quality Scores
              'relevance': metrics.get('relevance.relevance', metrics.get('relevance.gpt_relevance', 0)),
              'relevance_passing_rate': metrics.get('relevance.binary_aggregate', 0) * 100,
              
              'coherence': metrics.get('coherence.coherence', metrics.get('coherence.gpt_coherence', 0)),
              'coherence_passing_rate': metrics.get('coherence.binary_aggregate', 0) * 100,
              
              'fluency': metrics.get('fluency.fluency', metrics.get('fluency.gpt_fluency', 0)),
              'fluency_passing_rate': metrics.get('fluency.binary_aggregate', 0) * 100,
              
              'groundedness': metrics.get('groundedness.groundedness', metrics.get('groundedness.gpt_groundedness', 0)),
              'groundedness_passing_rate': metrics.get('groundedness.binary_aggregate', 0) * 100,
              
              'intent_resolution': metrics.get('intent_resolution.intent_resolution', 0),
              'intent_resolution_passing_rate': metrics.get('intent_resolution.binary_aggregate', 0) * 100,
              
              'task_adherence': metrics.get('task_adherence.task_adherence', 0),
              'task_adherence_passing_rate': metrics.get('task_adherence.binary_aggregate', 0) * 100,
              
              'tool_call_accuracy': metrics.get('tool_call_accuracy.tool_call_accuracy', 0),
              'tool_call_accuracy_passing_rate': metrics.get('tool_call_accuracy.binary_aggregate', 0) * 100,
              
              'similarity': metrics.get('similarity.similarity', metrics.get('similarity.gpt_similarity', 0)),
              'similarity_passing_rate': metrics.get('similarity.binary_aggregate', 0) * 100,
              
              # Operational Metrics
              'client_run_duration': metrics.get('operational_metrics.client-run-duration-in-seconds', 0),
              'server_run_duration': metrics.get('operational_metrics.server-run-duration-in-seconds', 0),
              'completion_tokens': metrics.get('operational_metrics.completion-tokens', 0),
              'prompt_tokens': metrics.get('operational_metrics.prompt-tokens', 0),
          }
          
          # Build enhanced summary for GitHub Actions
          summary = "# üìä Enhanced AI Quality Metrics\n\n"
          summary += "*Detailed breakdown from official Microsoft evaluation*\n\n"
          
          summary += "## üéØ AI Quality (AI-assisted)\n\n"
          summary += "| Evaluation Score | Value | Passing Rate | Status |\n"
          summary += "|------------------|-------|--------------|--------|\n"
          
          quality_metrics = [
              ('intent_resolution', 'Intent Resolution'),
              ('task_adherence', 'Task Adherence'),
              ('relevance', 'Relevance'),
              ('coherence', 'Coherence'),
              ('fluency', 'Fluency'),
              ('groundedness', 'Groundedness'),
              ('similarity', 'Similarity'),
              ('tool_call_accuracy', 'Tool Call Accuracy')
          ]
          
          for key, label in quality_metrics:
              value = extracted.get(key, 0)
              pass_rate = extracted.get(f'{key}_passing_rate', 0)
              
              # Status badge
              if pass_rate >= 100:
                  badge = "üü¢ Excellent"
              elif pass_rate >= 80:
                  badge = "üü° Good"
              elif pass_rate >= 60:
                  badge = "üü† Fair"
              else:
                  badge = "üî¥ Needs Improvement"
              
              summary += f"| {label} | {value:.2f} | {pass_rate:.1f}% | {badge} |\n"
          
          summary += "\n## ‚ö° Performance Metrics\n\n"
          summary += "| Metric | Value |\n"
          summary += "|--------|-------|\n"
          summary += f"| Client Run Duration | {extracted['client_run_duration']:.2f}s |\n"
          summary += f"| Server Run Duration | {extracted['server_run_duration']:.2f}s |\n"
          summary += f"| Completion Tokens | {extracted['completion_tokens']:.0f} |\n"
          summary += f"| Prompt Tokens | {extracted['prompt_tokens']:.0f} |\n"
          
          summary += "\n---\n"
          summary += f"**Agent ID:** `${{ steps.set-agents.outputs.agent_ids }}`\n"
          summary += f"**Evaluation File:** `{eval_file}`\n"
          
          # Write to GitHub Actions Summary
          with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
              f.write("\n\n" + summary)
          
          print("‚úÖ Enhanced metrics summary added to Actions tab")
          print("\nExtracted Metrics:")
          print(json.dumps(extracted, indent=2))
          EOF
      
      - name: Generate PR Comment
        if: always()
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            let comment = "## ü§ñ Agent Evaluation Results\n\n";
            
            if ("${{ steps.run-evaluation.outcome }}" === "success") {
              comment += "### ‚úÖ Evaluation Completed with Statistical Comparison\n\n";
              comment += "Both agents have been evaluated and statistically compared.\n\n";
              comment += "**View detailed results:**\n";
              comment += `- [GitHub Actions Summary](${context.payload.pull_request.html_url}/checks)\n`;
              comment += `- [Azure AI Foundry Portal](${process.env.AZURE_AI_PROJECT_ENDPOINT?.replace('/api/projects/', '/projects/')})\n\n`;
              comment += "### üìä Agents Compared\n\n";
              comment += `- **Baseline:** \`${{ vars.AGENT_ID_BASELINE }}\`\n`;
              comment += `- **Variant:** \`${{ vars.AGENT_ID_V2 }}\`\n\n`;
              comment += "The official action automatically calculates:\n";
              comment += "- Absolute values with confidence intervals\n";
              comment += "- Statistical significance tests\n";
              comment += "- Whether differences are meaningful or due to random variation\n";
            } else {
              comment += "### ‚ùå Evaluation Failed\n\n";
              comment += "The evaluation encountered an error. Please check the workflow logs.\n\n";
              comment += "**Common issue:** Safety evaluators (violence, hate, sexual) may return 'not applicable' strings that cannot be statistically compared.\n";
              comment += "**Solution:** Remove safety evaluators from your test data or use numeric-only evaluators.\n";
            }
            
            comment += "\n---\n";
            comment += `**Workflow:** [View logs](${context.payload.repository.html_url}/actions/runs/${context.runId})\n`;
            
            // Find and update or create comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number
            });
            
            const botComment = comments.find(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('ü§ñ Agent Evaluation Results')
            );
            
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }
      
      - name: Evaluation Summary
        if: success()
        run: |
          echo "‚úÖ Agent evaluation completed successfully!"
          echo ""
          echo "üìä Results have been posted to:"
          echo "   1. GitHub Actions Summary (see above)"
          echo "   2. PR Comment"
          echo "   3. Azure AI Foundry Portal"
          echo ""
          if [ "${{ steps.check-baseline.outputs.baseline_exists }}" == "true" ]; then
            echo "‚ÑπÔ∏è For statistical comparison with baseline:"
            echo "   Pass multiple agent IDs (baseline,variant) to the evaluation"
          else
            echo "üìù First evaluation run - will become baseline when merged"
          fi
