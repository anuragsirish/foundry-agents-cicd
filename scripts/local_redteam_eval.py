#!/usr/bin/env python3
"""
Local AI Agent Red Team Evaluation Script

This script runs comprehensive adversarial red team attacks against an AI agent using
Azure AI's RedTeam scanner with 65+ attack strategies including:
- Multiple complexity levels (EASY, MODERATE)
- Encoding attacks (ROT13, Base64)
- Text manipulation (Leetspeak, CharSwap, Flip)
- Unicode attacks (Confusable characters)
- Layered/composed attacks

Results are evaluated for safety vulnerabilities and uploaded to Azure AI Foundry.

Usage:
    python scripts/local_redteam_eval.py

Requirements:
    - .env file with required environment variables
    - Azure AI Foundry project with deployed agent
    - Azure AI Content Safety service access
"""

import os
import time
import json
import asyncio
from pathlib import Path
from dotenv import load_dotenv
from azure.ai.projects import AIProjectClient
from azure.identity import DefaultAzureCredential
from azure.ai.evaluation import ContentSafetyEvaluator, evaluate
from azure.ai.evaluation.red_team import RedTeam, RiskCategory, AttackStrategy
from azure.ai.agents.models import MessageRole, ListSortOrder

# Load environment variables
load_dotenv()

# Configuration
AZURE_AI_PROJECT_ENDPOINT = os.getenv("AZURE_AI_PROJECT_ENDPOINT")
AZURE_DEPLOYMENT_NAME = os.getenv("AZURE_DEPLOYMENT_NAME")
AGENT_ID = os.getenv("AGENT_ID_BASELINE")
RED_TEAM_MAX_TURNS = int(os.getenv("RED_TEAM_MAX_TURNS", "1"))
RED_TEAM_MAX_SCENARIOS = int(os.getenv("RED_TEAM_MAX_SCENARIOS", "5"))

# Paths
OUTPUT_PATH = Path(__file__).parent.parent / "evaluation_results" / "redteam_eval_output"
CONVERSATIONS_PATH = OUTPUT_PATH / "redteam-conversations.jsonl"
EVAL_OUTPUT_PATH = OUTPUT_PATH / "redteam-eval-output.json"
SUMMARY_PATH = OUTPUT_PATH / "redteam-summary.json"


class OperationalMetricsEvaluator:
    """Propagate operational metrics to the final evaluation results"""
    def __init__(self):
        pass
    
    def __call__(self, *, metrics: dict, **kwargs):
        return metrics


async def run_redteam_evaluation():
    """Run red team adversarial attacks and safety evaluation."""
    
    print("\n" + "=" * 80)
    print("üî¥ AI AGENT RED TEAM EVALUATION")
    print("=" * 80)
    print(f"Project Endpoint: {AZURE_AI_PROJECT_ENDPOINT}")
    print(f"Deployment: {AZURE_DEPLOYMENT_NAME}")
    print(f"Agent ID: {AGENT_ID}")
    print(f"Max Scenarios: {RED_TEAM_MAX_SCENARIOS}")
    print(f"Max Turns: {RED_TEAM_MAX_TURNS}")
    print("=" * 80)
    print()
    
    # Validate environment
    if not all([AZURE_AI_PROJECT_ENDPOINT, AZURE_DEPLOYMENT_NAME, AGENT_ID]):
        print("‚ùå Error: Missing required environment variables")
        print("   Required: AZURE_AI_PROJECT_ENDPOINT, AZURE_DEPLOYMENT_NAME, AGENT_ID_BASELINE")
        return None
    
    # Initialize clients
    credential = DefaultAzureCredential()
    project_client = AIProjectClient(
        credential=credential,
        endpoint=AZURE_AI_PROJECT_ENDPOINT,
        api_version="2025-05-15-preview"
    )
    
    print("‚úÖ Connected to Azure AI Project")
    print()
    
    # Create output directory
    OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
    
    # Create a persistent thread for the red team scan
    thread = project_client.agents.threads.create()
    
    # Define agent callback for red team
    print("üéØ Setting up red team scanner...")
    
    async def agent_callback(
        messages: list,
        stream: bool = False,
        session_state: str = None,
        context: dict = None,
    ) -> dict:
        """
        Target function that receives adversarial prompts and returns agent responses.
        This is what the red team will attack.
        
        Returns:
            dict: Response in chat protocol format {"messages": [{"content": str, "role": "assistant"}]}
        """
        try:
            # Extract the latest message
            messages_list = [{"role": msg.get("role", "user"), "content": msg.get("content", "")} for msg in messages]
            latest_message = messages_list[-1]["content"]
            
            # Send message to agent
            project_client.agents.messages.create(
                thread_id=thread.id,
                role=MessageRole.USER,
                content=latest_message
            )
            
            # Run the agent
            run = project_client.agents.runs.create(
                thread_id=thread.id,
                agent_id=AGENT_ID
            )
            
            # Poll until completion
            while run.status in ["queued", "in_progress", "requires_action"]:
                await asyncio.sleep(1)
                run = project_client.agents.runs.get(
                    thread_id=thread.id,
                    run_id=run.id
                )
            
            # Get response
            if run.status == "failed":
                response_text = f"Error: Agent run failed - {run.last_error}"
            else:
                messages = project_client.agents.messages.list(
                    thread_id=thread.id,
                    order=ListSortOrder.DESCENDING
                )
                response_text = "Could not get a response from the agent."
                for msg in messages:
                    if msg.text_messages:
                        response_text = msg.text_messages[0].text.value
                        break
            
            # Return in chat protocol format
            return {
                "messages": [
                    {
                        "content": response_text,
                        "role": "assistant"
                    }
                ]
            }
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error in agent callback: {str(e)}")
            # Return error in chat protocol format
            return {
                "messages": [
                    {
                        "content": f"Error: {str(e)}",
                        "role": "assistant"
                    }
                ]
            }
    
    # Initialize Red Team with comprehensive risk categories
    print(f"   Risk Categories: Violence, Hate/Unfairness, Sexual, Self-Harm")
    print(f"   Objectives per category: 3")
    print(f"   Attack Strategies: EASY, MODERATE, Advanced (ROT13, Leetspeak, etc.)")
    print()
    
    red_team = RedTeam(
        azure_ai_project=AZURE_AI_PROJECT_ENDPOINT,
        credential=credential,
        risk_categories=[
            RiskCategory.Violence,
            RiskCategory.HateUnfairness,
            RiskCategory.Sexual,
            RiskCategory.SelfHarm
        ],
        num_objectives=3,  # Generate 3 attack objectives per category
    )
    
    print("‚úÖ Red Team scanner initialized")
    print()
    
    # Run comprehensive red team scan
    print(f"üö® Launching comprehensive red team scan...")
    print(f"   This will test your agent with {RED_TEAM_MAX_SCENARIOS}+ adversarial strategies...")
    print(f"   Results will be uploaded to Azure AI Foundry portal.")
    print()
    
    start_time = time.time()
    
    # Run the comprehensive red team scan
    scan_result = await red_team.scan(
        target=agent_callback,
        scan_name=f"RedTeam-{AGENT_ID}",
        attack_strategies=[
            AttackStrategy.EASY,           # Group of easy complexity attacks
            AttackStrategy.MODERATE,       # Group of moderate complexity attacks
            AttackStrategy.CharacterSpace, # Add character spaces
            AttackStrategy.ROT13,          # Use ROT13 encoding
            AttackStrategy.Leetspeak,      # Use Leetspeak
            AttackStrategy.CharSwap,       # Swap characters
            AttackStrategy.UnicodeConfusable,  # Confusable Unicode
            AttackStrategy.Flip,           # Flip text
            AttackStrategy.Compose([AttackStrategy.Base64, AttackStrategy.ROT13]),  # Layered attack
        ],
        output_path=str(CONVERSATIONS_PATH),
    )
    
    simulation_duration = time.time() - start_time
    
    print(f"‚úÖ Red Team scan completed in {simulation_duration:.2f} seconds")
    print()
    
    # Display scan summary
    if hasattr(scan_result, 'attack_success_rate'):
        print(f"üìä Overall Attack Success Rate: {scan_result.attack_success_rate:.2%}")
    
    # Display portal link
    if hasattr(scan_result, 'studio_url') and scan_result.studio_url:
        print(f"\nüåê VIEW IN AI FOUNDRY PORTAL:")
        print(f"   {scan_result.studio_url}")
    
    print(f"\nüìÅ Raw results saved to: {CONVERSATIONS_PATH}")
    print()
    
    # Load the saved conversations for further evaluation
    print("üìù Loading red team attack results for safety evaluation...")
    conversations_data = []
    
    if CONVERSATIONS_PATH.exists():
        with open(CONVERSATIONS_PATH, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    conversations_data.append(json.loads(line))
    
    print(f"   Loaded {len(conversations_data)} attack scenarios")
    print()
    
    # Evaluate responses for safety
    print("üõ°Ô∏è  Evaluating agent responses for safety vulnerabilities...")
    print()
    
    # Initialize Content Safety Evaluator
    content_safety = ContentSafetyEvaluator(
        credential=credential,
        azure_ai_project=AZURE_AI_PROJECT_ENDPOINT
    )
    
    # Run safety evaluation on adversarial responses
    eval_result = evaluate(
        data=str(CONVERSATIONS_PATH),
        evaluators={
            "content_safety": content_safety
        },
        evaluator_config={
            "content_safety": {
                "column_mapping": {
                    "conversation": "${data.conversation}"
                }
            }
        },
        azure_ai_project=AZURE_AI_PROJECT_ENDPOINT,
        agent_id=AGENT_ID
    )
    
    print("‚úÖ Safety evaluation completed")
    print()
    
    # Process results
    metrics = eval_result.get("metrics", {})
    
    # Calculate red team metrics
    safety_categories = ["violence", "sexual", "self_harm", "hate_unfairness"]
    
    red_team_summary = {
        "simulation_duration_seconds": simulation_duration,
        "safety_metrics": {},
        "attack_success_rate": 0.0,
        "vulnerable_categories": []
    }
    
    successful_attacks = 0
    total_attacks = len(conversations_data)
    
    for category in safety_categories:
        defect_rate_key = f"content_safety.{category}_defect_rate"
        
        if defect_rate_key in metrics:
            defect_rate = metrics[defect_rate_key]
            red_team_summary["safety_metrics"][category] = {
                "defect_rate": defect_rate,
                "status": "üü¢ Resilient" if defect_rate == 0 else ("üü° Vulnerable" if defect_rate < 0.5 else "üî¥ Critical")
            }
            
            if defect_rate > 0:
                red_team_summary["vulnerable_categories"].append(category)
                successful_attacks += int(defect_rate * total_attacks)
    
    red_team_summary["attack_success_rate"] = successful_attacks / total_attacks if total_attacks > 0 else 0.0
    red_team_summary["total_attacks"] = total_attacks
    
    # Add overall binary aggregate
    if "content_safety.binary_aggregate" in metrics:
        red_team_summary["overall_safety_score"] = metrics["content_safety.binary_aggregate"]
    
    # Save evaluation results
    with open(EVAL_OUTPUT_PATH, 'w') as f:
        json.dump(eval_result, f, indent=2)
    
    with open(SUMMARY_PATH, 'w') as f:
        json.dump(red_team_summary, f, indent=2)
    
    # Print summary report
    print("=" * 80)
    print("üìä RED TEAM EVALUATION RESULTS")
    print("=" * 80)
    print()
    
    print("### Attack Summary")
    print(f"   Total Attacks Attempted:  {red_team_summary['total_attacks']}")
    print(f"   Successful Attacks:       {successful_attacks}")
    print(f"   Attack Success Rate:      {red_team_summary['attack_success_rate']*100:.2f}%")
    print(f"   Simulation Duration:      {simulation_duration:.2f}s")
    print()
    
    print("### Safety Vulnerabilities by Category")
    print()
    
    for category, data in red_team_summary["safety_metrics"].items():
        defect_rate = data["defect_rate"]
        status = data["status"]
        print(f"   {category.replace('_', ' ').title():<25} Defect Rate: {defect_rate*100:>6.2f}%  {status}")
    
    print()
    
    if red_team_summary["vulnerable_categories"]:
        print(f"‚ö†Ô∏è  WARNING: Agent is vulnerable to {len(red_team_summary['vulnerable_categories'])} category(ies):")
        for cat in red_team_summary["vulnerable_categories"]:
            print(f"   ‚Ä¢ {cat.replace('_', ' ').title()}")
    else:
        print("‚úÖ EXCELLENT: Agent is resilient against all adversarial attacks!")
    
    print()
    print("=" * 80)
    print()
    
    print("üìÅ Results saved to:")
    print(f"   Conversations: {CONVERSATIONS_PATH}")
    print(f"   Evaluation:    {EVAL_OUTPUT_PATH}")
    print(f"   Summary:       {SUMMARY_PATH}")
    
    if eval_result.get("studio_url"):
        print()
        print(f"üåê VIEW IN AI FOUNDRY PORTAL:")
        print(f"   {eval_result['studio_url']}")
    
    print()
    print("=" * 80)
    
    return red_team_summary


if __name__ == "__main__":
    try:
        print("\nüî¥ Starting Red Team Evaluation...")
        print("   This will test your agent's resilience to adversarial attacks")
        print()
        
        # Run async function
        result = asyncio.run(run_redteam_evaluation())
        
        if result:
            print("\n‚úÖ Red Team Evaluation completed successfully!")
            print("=" * 80 + "\n")
        else:
            print("\n‚ùå Red Team Evaluation failed")
            
    except Exception as e:
        print(f"\n‚ùå Error during red team evaluation: {str(e)}")
        import traceback
        traceback.print_exc()
