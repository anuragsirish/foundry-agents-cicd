# üöÄ AI Agent Deployment Guide: Dev ‚Üí Staging ‚Üí Production

> **Complete guide to deploying AI agents from development through staging to production with safety gates and monitoring**

---

## üìã Table of Contents

- [Overview](#overview)
- [The Three Environments](#the-three-environments)
- [The Promotion Process](#the-promotion-process)
- [Real-World Example](#real-world-example)
- [Safety Net Layers](#safety-net-layers)
- [Setup Instructions](#setup-instructions)
- [Deployment Workflows](#deployment-workflows)
- [Rollback Procedures](#rollback-procedures)
- [Monitoring & Alerts](#monitoring--alerts)
- [Best Practices](#best-practices)

---

## üéØ Overview

This guide explains how to safely promote AI agents from development to production using a **three-stage deployment process**:

```
Development ‚Üí Staging ‚Üí Production
    ‚Üì            ‚Üì           ‚Üì
  Testing    Validation   Live Users
```

### Why Three Stages?

| Stage | Purpose | Risk Level | Speed |
|-------|---------|------------|-------|
| **Dev** | Rapid iteration, experimentation | High (expected failures) | Fast |
| **Staging** | Final validation before production | Medium (should be stable) | Moderate |
| **Production** | Serve real users | Low (must be stable) | Slow (careful rollout) |

---

## üèóÔ∏è The Three Environments

### 1Ô∏è‚É£ Development Environment

**Purpose**: Where developers actively work and test changes

**Characteristics**:
- **Users**: Developers, QA engineers
- **Stability**: Frequently broken, rapid changes expected
- **Data**: Fake/synthetic test data
- **Agent IDs**: `AGENT_ID_BASELINE`, `AGENT_ID_V2` (for A/B testing)
- **Evaluation**: Full suite runs on every PR
- **Duration**: Minutes to hours

**When code changes**:
```bash
1. Developer creates feature branch
2. Opens PR ‚Üí Triggers CI/CD evaluation
3. Quality + Safety + Red Team tests run
4. Results posted to PR for review
5. If approved ‚Üí Merge to main branch
```

---

### 2Ô∏è‚É£ Staging Environment (Pre-production)

**Purpose**: Mirror of production for final validation

**Characteristics**:
- **Users**: QA team, Product managers, Stakeholders
- **Stability**: Should be stable, matches production configuration
- **Data**: Sanitized production data or realistic test data
- **Agent ID**: `AGENT_ID_STAGING`
- **Evaluation**: Extended test suite + manual testing
- **Duration**: 24-48 hours (soak testing)

**What happens in staging**:
```bash
1. Automated deployment from main branch
2. Smoke tests (2-5 minutes)
3. Integration tests (15-30 minutes)
4. User acceptance testing (UAT) - manual validation
5. Performance/load testing
6. Soak testing (24-48 hours monitoring)
7. Stakeholder approval gate
```

**Staging Checklist**:
- [ ] Smoke tests pass
- [ ] Integration tests pass
- [ ] UAT completed by product team
- [ ] Performance metrics acceptable
- [ ] No errors in 48-hour soak test
- [ ] Security review completed (if needed)
- [ ] Stakeholder sign-off received

---

### 3Ô∏è‚É£ Production Environment

**Purpose**: Live system serving real users

**Characteristics**:
- **Users**: End users, customers
- **Stability**: Must be highly stable and reliable
- **Data**: Real user data (protected)
- **Agent ID**: `AGENT_ID_PRODUCTION`
- **Evaluation**: Continuous monitoring + periodic deep evaluation
- **Duration**: Gradual rollout over hours/days

**Production deployment strategies**:

#### **Option A: Blue-Green Deployment** ‚úÖ Recommended
```
1. Current agent (Blue) serves 100% traffic
2. Deploy new agent (Green) to production (inactive)
3. Run smoke tests on Green
4. Switch 10% traffic to Green ‚Üí Monitor 1 hour
5. Switch 50% traffic to Green ‚Üí Monitor 2 hours
6. Switch 100% traffic to Green
7. Keep Blue warm for 24 hours (quick rollback)
```

#### **Option B: Canary Deployment**
```
1. Deploy to 5% of users ‚Üí Monitor 1 hour
2. Deploy to 25% of users ‚Üí Monitor 2 hours
3. Deploy to 100% of users
```

#### **Option C: Manual Approval Gate** (Safest for critical systems)
```
1. Deploy new agent (inactive)
2. Run production smoke tests
3. STOP: Require human approval
4. Manual review of all metrics
5. Human clicks "Approve" button
6. Agent activated for all users
```

---

## üîÑ The Promotion Process (Step-by-Step)

### **Week 1: Development Phase**

```
Developer's Laptop ‚Üí Dev Environment ‚Üí PR ‚Üí CI/CD
```

#### Day 1-2: Local Development
```bash
# Developer creates new agent feature locally
python agent-setup/create_agent_v2.py

# Test locally
python agent-setup/test_agent_locally.py
```

#### Day 3: Open Pull Request
```bash
git checkout -b feature/new-inventory-tool
git push origin feature/new-inventory-tool

# Open PR on GitHub ‚Üí Triggers workflow
```

#### Day 3-4: CI/CD Evaluation (Automatic)
```yaml
# .github/workflows/agent-evaluation-unified.yml runs:
- Quality Evaluation (8 evaluators)
  ‚úÖ Relevance, Coherence, Fluency, Groundedness
  ‚úÖ Similarity, Intent Resolution, Task Adherence
  ‚úÖ Tool Call Accuracy

- Safety Evaluation (4 risk categories)
  ‚úÖ Violence, Sexual, Self-harm, Hate/Unfairness
  ‚úÖ Content Safety Evaluator

- Red Team Testing (10+ attack strategies per risk category)
  ‚úÖ ROT13, Leetspeak, Base64, Unicode attacks
  ‚úÖ Attack strategy breakdown by risk category
  ‚úÖ Attack success rate measured
```

#### Day 4-5: Code Review + Approval
```
1. Team reviews code changes
2. Reviews evaluation results in PR
3. Discussion of any regressions
4. Approval from 2+ reviewers
5. PR merged to main branch
```

---

### **Week 2: Staging Deployment**

```
Main Branch ‚Üí Deploy to Staging ‚Üí Extended Testing
```

#### Day 1: Automated Staging Deployment
```yaml
# Triggered automatically on merge to main
name: Deploy to Staging
on:
  push:
    branches: [main]

jobs:
  deploy-staging:
    - Update AGENT_ID_STAGING with new code
    - Run smoke tests
    - Run integration tests
```

#### Day 1 (Continued): Smoke Tests
```python
# Quick validation (2-5 minutes)
def smoke_test():
    # 1. Agent exists
    agent = client.agents.get(AGENT_ID_STAGING)
    assert agent is not None
    
    # 2. Agent responds
    response = agent.run("Hello, are you working?")
    assert len(response) > 0
    
    # 3. Response time acceptable
    assert response_time < 5_seconds
    
    # 4. Basic tool calls work
    response = agent.run("Check inventory for SKU-123")
    assert "inventory" in response.lower()
```

#### Day 2-3: Integration & UAT
```bash
# Integration Tests (15-30 minutes)
- Test all API integrations
- Verify database connections
- Check authentication flows
- Test all tool functions

# User Acceptance Testing (Manual)
- Product team validates features
- Test edge cases and error handling
- Verify user experience
- Check business logic correctness
```

#### Day 4-5: Soak Testing
```bash
# Let agent run in staging for 48 hours
# Monitor for:
- Memory leaks
- Performance degradation over time
- Unexpected errors in logs
- Token usage trends
- Response time consistency
```

#### Day 6: Stakeholder Approval
```
Approval Checklist:
‚úÖ Product Manager: Features work as expected
‚úÖ QA Lead: All tests passed
‚úÖ Security Team: No security concerns
‚úÖ Engineering Lead: Technical review complete
‚úÖ Operations: Monitoring configured
```

---

### **Week 3: Production Deployment**

```
Staging (Approved) ‚Üí Production ‚Üí Gradual Rollout ‚Üí Monitor
```

#### Monday 10:00 AM: Pre-Deployment Meeting
```
Review Checklist:
‚úÖ All staging tests passed
‚úÖ Stakeholder approvals received
‚úÖ Deployment runbook prepared
‚úÖ Rollback plan documented
‚úÖ On-call engineer notified and ready
‚úÖ Customer support team briefed
‚úÖ Monitoring dashboards prepared
```

#### Monday 10:30 AM: Deploy to Production
```bash
# Step 1: Create new production agent (inactive)
az ai agent create \
  --name "customer-service-agent-v2" \
  --instructions "Updated instructions..." \
  --tools "new_tools.json"

# Step 2: Run production smoke tests
python scripts/production_smoke_test.py

# Output:
‚úÖ Agent created successfully
‚úÖ Agent responds to test queries
‚úÖ Tools functioning correctly
‚úÖ Response time: 1.2s (acceptable)
```

#### Monday 11:00 AM: Gradual Rollout (10%)
```bash
# Route 10% of traffic to new agent
# Monitor for 1 hour

Metrics to watch:
- Response time: ‚úÖ 1.3s avg (baseline: 1.2s)
- Error rate: ‚úÖ 0.02% (baseline: 0.01%)
- User satisfaction: ‚úÖ 4.8/5 (baseline: 4.7/5)
- Quality scores: ‚úÖ All metrics stable
```

#### Monday 12:00 PM: Increase to 50%
```bash
# Monitor for 2 hours

Hour 1:
‚úÖ Response time: 1.3s avg
‚úÖ Error rate: 0.02%
‚úÖ Quality evals: No regressions

Hour 2:
‚úÖ Response time: 1.2s avg
‚úÖ Error rate: 0.01%
‚úÖ User feedback: Positive
```

#### Monday 2:00 PM: Full Rollout (100%)
```bash
# All users now on new agent
# Keep old agent on standby for 24 hours

Monitoring intensifies:
- Real-time dashboard watching
- Alert system active
- On-call engineer monitoring
```

#### Monday 2:00 PM - Tuesday 2:00 PM: Intensive Monitoring
```bash
# Continuous evaluation every 4 hours
# Check:
- Quality metrics
- Safety scores
- Red team resilience
- Operational metrics
- User feedback

All Clear After 24 Hours:
‚úÖ No incidents
‚úÖ Performance stable
‚úÖ Quality maintained
‚úÖ User satisfaction high
```

#### Tuesday 2:00 PM: Deployment Complete
```bash
# Success criteria met:
‚úÖ 24 hours without incidents
‚úÖ All metrics stable or improved
‚úÖ No user complaints
‚úÖ Evaluation scores maintained

# Archive old agent (keep for 30 days for rollback)
```

---

## üìä Real-World Example

### Scenario: Adding "check_inventory" tool to customer service agent

#### **Phase 1: Development (Week 1)**

**Day 1-2**: Local development
```python
# Developer adds new inventory tool
def check_inventory(sku: str) -> dict:
    """Check if product is in stock."""
    response = inventory_api.get_stock(sku)
    return {
        "sku": sku,
        "in_stock": response.available > 0,
        "quantity": response.available,
        "price": response.price
    }
```

**Day 3**: Open PR
```bash
git commit -m "Add inventory checking tool"
git push origin feature/inventory-tool
# Open PR #123 on GitHub
```

**Day 3 (Evening)**: CI/CD runs automatically
```
ü§ñ Comprehensive Agent Evaluation

Quality Metrics:
‚úÖ Relevance: 5.00 ‚Üí 5.00 (no change)
‚úÖ Coherence: 5.00 ‚Üí 5.00 (no change)
‚úÖ Task Adherence: 5.00 ‚Üí 5.00 (no change)
‚úÖ Tool Call Accuracy: 1.00 ‚Üí 1.00 (perfect)

Safety Metrics:
‚úÖ Violence: 0.0% defect rate
‚úÖ Sexual: 0.0% defect rate
‚úÖ Self-harm: 0.0% defect rate
‚úÖ Hate/Unfairness: 0.0% defect rate

Red Team:
‚úÖ Attack Success Rate: 12% (acceptable)
‚úÖ 40 attack strategies tested
‚úÖ No vulnerable categories

üéØ Attack Strategy Breakdown:
| Risk Category | Attack Strategy | Baseline | V2 |
|---------------|-----------------|----------|-----|
| Violence | Easy | 30 | 30 |
| Violence | Moderate | 9 | 9 |
| Sexual | Leetspeak | 9 | 9 |
| Self Harm | Unicode Confusable | 9 | 9 |
| Hate Unfairness | Base64 | 9 | 9 |

Summary: ‚úÖ No regressions detected
```

**Day 4**: Code review
```
Reviewer 1: LGTM! Evals look great.
Reviewer 2: Approved. Tool implementation is clean.
Product Manager: Perfect! This is what we need.
```

**Day 5**: PR merged to `main`

---

#### **Phase 2: Staging (Week 2)**

**Day 1 Morning**: Auto-deploy to staging
```bash
# Workflow triggered automatically
Deploy to Staging:
‚úÖ Agent updated with inventory tool
‚úÖ Smoke tests passed (2 minutes)
‚úÖ Agent responds correctly
‚úÖ Inventory API connected successfully
```

**Day 1 Afternoon**: Integration tests
```bash
Integration Test Suite:
‚úÖ Test 1: Valid SKU returns stock info
‚úÖ Test 2: Invalid SKU returns error gracefully
‚úÖ Test 3: Out of stock products handled correctly
‚úÖ Test 4: API timeout handled gracefully
‚úÖ Test 5: Multiple concurrent requests work
```

**Day 2-3**: User Acceptance Testing (UAT)
```
QA Team Testing:
‚úÖ "Is SKU-12345 in stock?" ‚Üí Correct response
‚úÖ "What's the price of XYZ-789?" ‚Üí Returns price
‚úÖ "Check availability of ABC-999" ‚Üí Out of stock handled
‚úÖ Edge cases tested: Invalid SKUs, special characters
‚úÖ User experience validated

Product Manager Validation:
‚úÖ Feature works as expected
‚úÖ Response format is user-friendly
‚úÖ Error messages are clear
```

**Day 4-5**: Soak testing (48 hours)
```bash
Monitoring Results (48 hours):
‚úÖ 2,450 requests processed
‚úÖ Average response time: 1.8s
‚úÖ Error rate: 0.02% (3 errors, all handled gracefully)
‚úÖ Memory usage: Stable at 512MB
‚úÖ No crashes or exceptions
‚úÖ Quality scores: Consistent
```

**Day 6**: Approval gate
```
Approvals Received:
‚úÖ Product Manager: Approved
‚úÖ QA Lead: Approved
‚úÖ Security Team: No concerns
‚úÖ Engineering Lead: Approved

Status: READY FOR PRODUCTION
```

---

#### **Phase 3: Production (Week 3)**

**Monday 10:00 AM**: Deployment meeting
```
Team assembles for deployment:
- Product Manager (stakeholder)
- Engineering Lead (technical oversight)
- DevOps Engineer (deployment execution)
- On-Call Engineer (monitoring)
- Customer Support Lead (user impact)

Checklist Review: ALL ‚úÖ
Decision: PROCEED WITH DEPLOYMENT
```

**Monday 10:30 AM**: Deploy new agent
```bash
# Create production agent v2
AGENT_ID_PRODUCTION_V2 created successfully

# Run smoke tests
‚úÖ Agent responds: 1.1s
‚úÖ Inventory tool works: Stock check successful
‚úÖ Error handling: Graceful failures
‚úÖ All systems green
```

**Monday 11:00 AM**: 10% rollout
```
Traffic Split:
- 90% ‚Üí Old agent (stable)
- 10% ‚Üí New agent (testing)

Monitoring (1 hour):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Metric              ‚îÇ Old     ‚îÇ New      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Avg Response Time   ‚îÇ 1.2s    ‚îÇ 1.3s  ‚úÖ ‚îÇ
‚îÇ Error Rate          ‚îÇ 0.01%   ‚îÇ 0.02% ‚úÖ ‚îÇ
‚îÇ Quality Score       ‚îÇ 4.9/5   ‚îÇ 4.9/5 ‚úÖ ‚îÇ
‚îÇ User Satisfaction   ‚îÇ 4.7/5   ‚îÇ 4.8/5 ‚úÖ ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Decision: Metrics acceptable, proceed to 50%
```

**Monday 12:00 PM**: 50% rollout
```
Traffic Split:
- 50% ‚Üí Old agent
- 50% ‚Üí New agent

Monitoring (Hour 1):
‚úÖ Response time stable: 1.2s avg
‚úÖ Error rate: 0.01%
‚úÖ User queries with inventory: 85% success rate
‚úÖ No customer complaints

Monitoring (Hour 2):
‚úÖ All metrics green
‚úÖ Inventory tool used 247 times successfully
‚úÖ Customer support: No issues reported
‚úÖ Quality evaluations: No regressions

Decision: Proceed to 100%
```

**Monday 2:00 PM**: 100% rollout
```
Traffic Split:
- 100% ‚Üí New agent ‚úÖ
- Old agent on standby (24 hour safety net)

All Users Now on New Agent!

Intensive Monitoring Begins:
- Dashboard: Real-time metrics visible
- Alerts: Configured for anomalies
- On-call: Engineer actively monitoring
- Support: Team briefed and ready
```

**Monday 2:00 PM - Tuesday 2:00 PM**: 24-hour watch
```
Hour 2:  ‚úÖ All green
Hour 4:  ‚úÖ Quality eval run - no regressions
Hour 6:  ‚úÖ All green
Hour 8:  ‚úÖ Quality eval run - all passing
Hour 12: ‚úÖ All green
Hour 16: ‚úÖ Quality eval run - all passing
Hour 20: ‚úÖ All green
Hour 24: ‚úÖ Quality eval run - all passing

Summary (24 hours):
- 12,450 requests processed
- 0 critical errors
- 0 customer complaints
- Quality scores: Maintained at 4.9/5
- Inventory tool: 3,200+ successful uses
- Customer satisfaction: Increased to 4.8/5

Status: ‚úÖ DEPLOYMENT SUCCESSFUL
```

**Tuesday 2:00 PM**: Close deployment
```bash
# Archive old agent
az ai agent archive --id $AGENT_ID_PRODUCTION_OLD

# Keep for 30 days (rollback safety)
# Update documentation
# Post-deployment report generated

üéâ DEPLOYMENT COMPLETE

New Feature Live:
- Customers can now check inventory in real-time
- 3,200+ successful inventory checks in first 24 hours
- No incidents or degradation
- Quality maintained across all metrics
```

---

## üõ°Ô∏è Safety Net Layers

### Why Multiple Testing Stages?

Each stage catches different types of issues:

| Stage | What It Catches | Example |
|-------|-----------------|---------|
| **PR Evaluation** | Code quality, obvious bugs | Agent hallucinating product info |
| **Staging Smoke Tests** | Deployment issues | Wrong environment variables |
| **Staging Integration** | System integration problems | Inventory API auth failing |
| **Staging Soak** | Performance degradation | Memory leak after 1000 requests |
| **Production Canary** | Real-world edge cases | Unexpected user query patterns |
| **Production Monitor** | Production-only issues | Peak traffic performance problems |

### Cost of Finding Bugs at Each Stage

```
üí∞ $1      - Fix in development (PR caught it)
üí∞üí∞ $10     - Fix in staging (QA caught it)
üí∞üí∞üí∞ $100    - Fix in production (user reported it)
üí∞üí∞üí∞üí∞ $1,000  - Production outage (all users affected)
üí∞üí∞üí∞üí∞üí∞ $10,000 - Data breach or safety incident
```

**Lesson**: Invest in earlier stages to save money and reputation.

---

## üîß Setup Instructions

### Prerequisites

```bash
# 1. Azure AI resources for each environment
- Dev: Azure AI Project (existing)
- Staging: Azure AI Project (new) or separate agents in same project
- Production: Azure AI Project (separate subscription recommended)

# 2. GitHub repository variables
- AGENT_ID_BASELINE (dev - existing)
- AGENT_ID_V2 (dev - existing)
- AGENT_ID_STAGING (new)
- AGENT_ID_PRODUCTION (new)
- AZURE_AI_PROJECT_ENDPOINT_STAGING (new)
- AZURE_AI_PROJECT_ENDPOINT_PRODUCTION (new)

# 3. GitHub Environments (for approval gates)
- staging
- production
```

### Step 1: Create Staging Environment

```bash
# Create staging agent in Azure AI Foundry
az ai agent create \
  --name "customer-service-agent-staging" \
  --instructions "@agent-setup/instructions.txt" \
  --model "gpt-4o" \
  --tools "@agent-setup/tools.json"

# Note the agent ID
export AGENT_ID_STAGING="asst_xxxxxxxxxxxxx"
```

### Step 2: Create Production Environment

```bash
# Create production agent (separate subscription recommended)
az ai agent create \
  --name "customer-service-agent-production" \
  --instructions "@agent-setup/instructions.txt" \
  --model "gpt-4o" \
  --tools "@agent-setup/tools.json"

# Note the agent ID
export AGENT_ID_PRODUCTION="asst_yyyyyyyyyyyyy"
```

### Step 3: Configure GitHub Variables

```bash
# Go to: Settings ‚Üí Secrets and variables ‚Üí Actions ‚Üí Variables

Add repository variables:
- AGENT_ID_STAGING = asst_xxxxxxxxxxxxx
- AGENT_ID_PRODUCTION = asst_yyyyyyyyyyyyy
- AZURE_AI_PROJECT_ENDPOINT_STAGING = https://staging-project.services.ai.azure.com/
- AZURE_AI_PROJECT_ENDPOINT_PRODUCTION = https://prod-project.services.ai.azure.com/
```

### Step 4: Create GitHub Environments (Approval Gates)

```bash
# Go to: Settings ‚Üí Environments

Create "staging" environment:
- Deployment branches: main only
- No approval required (automated)

Create "production" environment:
- Deployment branches: main only
- Required reviewers: 2 people minimum
- Add: Product Manager, Engineering Lead
- Wait timer: 0 minutes (manual trigger)
```

---

## üöÄ Deployment Workflows

### Current State (PR Evaluations Only)
```yaml
.github/workflows/
  ‚îî‚îÄ‚îÄ agent-evaluation-unified.yml (PR evaluations with quality + safety + red team)
```

### Target State (Full CI/CD Pipeline)
```yaml
.github/workflows/
  ‚îú‚îÄ‚îÄ agent-evaluation-unified.yml      # PR evaluations (quality, safety, red team)
  ‚îú‚îÄ‚îÄ deploy-to-staging.yml             # Auto-deploy to staging
  ‚îú‚îÄ‚îÄ deploy-to-production.yml          # Manual deploy to production
  ‚îú‚îÄ‚îÄ production-monitoring.yml         # Continuous monitoring
  ‚îî‚îÄ‚îÄ rollback-production.yml           # Emergency rollback
```

---

## üîÑ Rollback Procedures

### When to Rollback

Rollback immediately if:
- ‚ùå Error rate increases > 5%
- ‚ùå Response time increases > 50%
- ‚ùå Quality scores drop > 10%
- ‚ùå Customer complaints spike
- ‚ùå Safety violations detected
- ‚ùå Critical bug discovered

### Rollback Methods

#### **Method 1: Traffic Switch (Instant)**
```bash
# If using Blue-Green deployment
# Just switch traffic back to old agent
# Takes: < 1 minute

# Azure Portal or CLI:
az ai agent set-traffic --id $AGENT_ID_OLD --percentage 100
```

#### **Method 2: Agent Replacement (Fast)**
```bash
# Replace production agent with previous version
# Takes: 2-5 minutes

python scripts/rollback_to_baseline.py \
  --production-agent-id $AGENT_ID_PRODUCTION \
  --rollback-to $AGENT_ID_BASELINE
```

#### **Method 3: Re-deploy Old Code (Slow)**
```bash
# Trigger deployment workflow with old commit
# Takes: 10-15 minutes

gh workflow run deploy-to-production.yml \
  --ref <old-commit-sha>
```

---

## üìä Monitoring & Alerts

### Production Monitoring Dashboard

**Metrics to Track**:

```yaml
Response Time:
  - Target: < 2 seconds
  - Warning: > 3 seconds
  - Critical: > 5 seconds

Error Rate:
  - Target: < 0.1%
  - Warning: > 0.5%
  - Critical: > 1%

Quality Scores (eval every 4 hours):
  - Target: > 4.5/5
  - Warning: < 4.0/5
  - Critical: < 3.5/5

Safety Defect Rate:
  - Target: 0%
  - Warning: > 0.1%
  - Critical: > 1%

Token Usage:
  - Track: Avg tokens per request
  - Alert: If increases > 50%
```

### Alert Configuration

```yaml
# .github/workflows/production-monitoring.yml
name: Production Monitoring
on:
  schedule:
    - cron: '0 */4 * * *'  # Every 4 hours

jobs:
  monitor:
    steps:
      - Run evaluations
      - Check metrics
      - If degradation detected:
          ‚Üí Send Slack alert
          ‚Üí Notify on-call engineer
          ‚Üí Post to GitHub issue
```

---

## ‚úÖ Best Practices

### Do's ‚úÖ

1. **Always test in staging first** - Never skip staging
2. **Use gradual rollouts** - Start with 10%, then 50%, then 100%
3. **Monitor intensively** - Watch metrics closely during deployment
4. **Keep rollback plan ready** - Be prepared to revert quickly
5. **Document everything** - Maintain deployment runbooks
6. **Communicate proactively** - Keep stakeholders informed
7. **Run continuous evaluations** - Quality, safety, red team tests
8. **Set up alerts** - Automated notifications for issues
9. **Maintain audit trail** - Track all changes and approvals
10. **Practice deployments** - Regular dry runs reduce risk

### Don'ts ‚ùå

1. **Don't deploy on Fridays** - Give yourself time to fix issues
2. **Don't skip smoke tests** - Always validate after deployment
3. **Don't deploy during peak hours** - Choose low-traffic windows
4. **Don't auto-deploy to production** - Always require human approval
5. **Don't ignore small regressions** - They compound over time
6. **Don't rush deployments** - Follow the process
7. **Don't deploy without backup** - Always have rollback ready
8. **Don't ignore staging issues** - Fix them before production
9. **Don't deploy untested code** - All code must pass evaluations
10. **Don't forget monitoring** - Track metrics after deployment

### AI Agent-Specific Considerations

```yaml
Special Considerations for AI Agents:

1. Non-Deterministic Behavior:
   - Run evaluations multiple times
   - Use statistical thresholds
   - Monitor variance in responses

2. Prompt Changes:
   - Test extensively in staging
   - Small prompt changes can have big impacts
   - Always A/B test in production

3. Tool/Function Changes:
   - Verify all integrations work
   - Test error handling thoroughly
   - Monitor tool call accuracy

4. Safety is Critical:
   - Never skip safety evaluations
   - Red team testing is mandatory
   - Monitor for adversarial inputs

5. User Trust:
   - Bad AI responses damage trust quickly
   - Quality must remain consistently high
   - Transparent communication about changes
```

---

## üìö Related Documentation

- [CI/CD Pipeline Guide](CICD_PIPELINE.md) - Overview of evaluation pipeline
- [Setup Guide](SETUP-GUIDE.md) - Initial repository setup
- [Architecture](ARCHITECTURE.md) - System architecture
- [Quick Reference](QUICK_REFERENCE.md) - Common commands
- [README](README.md) - Project overview

---

## ü§ù Support

If you have questions about the deployment process:
1. Review this guide thoroughly
2. Check related documentation
3. Consult with your DevOps team
4. Reach out to engineering leadership

---

## üìù Changelog

**Version 1.0** (October 2025)
- Comprehensive deployment guide created
- Three-stage process documented
- Real-world examples included
- Best practices compiled
- Reflects comprehensive evaluation pipeline (quality + safety + red team)

---

**Remember**: The goal is to deploy safely and confidently. Take your time, follow the process, and your deployments will be successful! üöÄ
